{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Maximum Contiguous Subsequence Sum Using Deep Learning\n",
    "## 问题描述\n",
    "\n",
    "设计一个网络结构，解决如下问题：输入一个sequence和一个标量query，找出该序列中连续query个数相加最大的子序列的起始位置\n",
    "\n",
    "Constraints：\n",
    "1. batch = 32, lr=0.01, optimizer=Adam，只能跑一个epoch\n",
    "2. 为避免直接利用数字信息，将sequence和query统一embed到8维空间作为输入\n",
    "\n",
    "考察指标\n",
    "最后100个batch的平均准确率大于96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1. Label数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9\n",
       "0  6  7  5  9  1  2  8  0  4  3\n",
       "1  2  0  3  7  8  5  9  1  6  4\n",
       "2  9  3  2  0  7  1  6  5  8  4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = pd.read_csv('data/task3_passage.csv', header=None)\n",
    "sequences.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  3\n",
       "1  5\n",
       "2  9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = pd.read_csv('data/task3_query.csv', header=None)\n",
    "queries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  0\n",
       "0  6  7  5  9  1  2  8  0  4  3  3\n",
       "1  2  0  3  7  8  5  9  1  6  4  5\n",
       "2  9  3  2  0  7  1  6  5  8  4  9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs_concat = pd.concat([sequences, queries], axis=1)  # concat data for easy processing\n",
    "seqs_concat.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_max_subseq_i(row):\n",
    "    '''\n",
    "    Find contiguous subsequence with maximum sum using 1-layer convolution\n",
    "    '''\n",
    "    seq = torch.tensor(row.iloc[:-1].values, dtype=torch.float).view(1, 1, -1)  # retrieve and reshape original seq to 3 dim\n",
    "    query = row.iloc[-1]  # retrieve query\n",
    "    conv = nn.Conv1d(1, 1, query, bias=False)  # build convolution with kernel size 3\n",
    "    conv.weight.data.fill_(torch.tensor(1))  # fill fixed value 1 into conv weight\n",
    "    sum_ = conv(seq)\n",
    "    \n",
    "    return sum_.argmax().item()  # get max starting index\n",
    "\n",
    "labels = seqs_concat.apply(find_max_subseq_i, axis=1)\n",
    "labels.to_csv(\"data/task3_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def oneHot(x, length=11):\n",
    "    tensor = torch.zeros(length)\n",
    "    tensor[x+1] = 1\n",
    "#     print(tensor)\n",
    "    return tensor\n",
    "\n",
    "# one_hot_labels = labels.apply(oneHot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = list(zip(seqs_concat.values, labels.values))\n",
    "BATCH_SIZE = 32\n",
    "trainloader=DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2. 设计网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define hyperparameter\n",
    "EMBEDDING_DIM = 8\n",
    "HIDDEN_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMIdxer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, idx_size, batch_size):\n",
    "        super(LSTMIdxer, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to index space\n",
    "        self.hidden2idx = nn.Linear(hidden_dim, idx_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, seq, target_loss=\"CrossEntropyLoss\"):\n",
    "        embeds = self.embeddings(seq)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) \n",
    "#         idx_space = self.hidden2idx(self.hidden[0])[0]\n",
    "        idx_space = self.hidden2idx(self.hidden[0]).unsqueeze(0)\n",
    "        if target_loss == \"CrossEntropyLoss\":\n",
    "            return F.log_softmax(idx_space, dim=1)\n",
    "        elif target_loss == \"MSELoss\":\n",
    "            return idx_space\n",
    "#         return idx_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, loss_function, name_scope, epochs=1):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            inputs, idx = data\n",
    "            # Step 3. Run our forward pass.\n",
    "            if type(loss_function) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "                idx_scores = model(inputs, \"CrossEntropyLoss\")\n",
    "                predicted = torch.argmax(idx_scores, 1)\n",
    "            elif type(loss_function) == torch.nn.modules.loss.MSELoss:\n",
    "                idx =idx.float()  # cast to float\n",
    "                idx_scores = model(inputs, \"MSELoss\").view(idx.size())\n",
    "                predicted = torch.round(idx_scores)\n",
    "            total_correct += (predicted == idx).sum().item()\n",
    "            total_sample += idx_scores.size(0)\n",
    "            \n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(idx_scores, idx)\n",
    "            total_loss += loss\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct/total_sample\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                writer.add_scalar(name_scope+'/avg_loss', avg_loss, ith)\n",
    "                writer.add_scalar(name_scope+'/avg_acc', avg_acc, ith)\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM with cross entropy Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMIdxer(\n",
      "  (embeddings): Embedding(11, 8)\n",
      "  (lstm): LSTM(8, 64, batch_first=True)\n",
      "  (hidden2idx): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_cr = LSTMIdxer(EMBEDDING_DIM, HIDDEN_DIM, 11, 10, BATCH_SIZE)\n",
    "cr_loss = nn.CrossEntropyLoss()\n",
    "print(lstm_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 0.7154936790466309, average accuracy: 0.718125\n",
      "1th 100 batches, average loss: 0.679318904876709, average accuracy: 0.7296875\n",
      "2th 100 batches, average loss: 0.650575578212738, average accuracy: 0.735625\n",
      "3th 100 batches, average loss: 0.6050158739089966, average accuracy: 0.77125\n",
      "4th 100 batches, average loss: 0.6724005341529846, average accuracy: 0.7340625\n",
      "5th 100 batches, average loss: 0.6295040249824524, average accuracy: 0.7540625\n",
      "6th 100 batches, average loss: 0.5945451855659485, average accuracy: 0.770625\n",
      "7th 100 batches, average loss: 0.632266104221344, average accuracy: 0.7575\n",
      "8th 100 batches, average loss: 0.6017310619354248, average accuracy: 0.7615625\n",
      "9th 100 batches, average loss: 0.5905725955963135, average accuracy: 0.7671875\n",
      "10th 100 batches, average loss: 0.5930705666542053, average accuracy: 0.768125\n",
      "11th 100 batches, average loss: 0.5778584480285645, average accuracy: 0.7753125\n",
      "12th 100 batches, average loss: 0.5924740433692932, average accuracy: 0.7715625\n",
      "13th 100 batches, average loss: 0.5729159712791443, average accuracy: 0.7753125\n",
      "14th 100 batches, average loss: 0.544258177280426, average accuracy: 0.7946875\n",
      "15th 100 batches, average loss: 0.5418500304222107, average accuracy: 0.793125\n",
      "16th 100 batches, average loss: 0.5362205505371094, average accuracy: 0.7940625\n",
      "17th 100 batches, average loss: 0.5299299955368042, average accuracy: 0.7878125\n",
      "18th 100 batches, average loss: 0.5470209717750549, average accuracy: 0.78375\n",
      "19th 100 batches, average loss: 0.527197539806366, average accuracy: 0.79125\n",
      "20th 100 batches, average loss: 0.5134889483451843, average accuracy: 0.81\n",
      "21th 100 batches, average loss: 0.5348446369171143, average accuracy: 0.7925\n",
      "22th 100 batches, average loss: 0.5240689516067505, average accuracy: 0.795625\n",
      "23th 100 batches, average loss: 0.492281049489975, average accuracy: 0.8103125\n",
      "24th 100 batches, average loss: 0.4972700774669647, average accuracy: 0.81625\n",
      "25th 100 batches, average loss: 0.49362096190452576, average accuracy: 0.8128125\n",
      "26th 100 batches, average loss: 0.520143985748291, average accuracy: 0.7971875\n",
      "27th 100 batches, average loss: 0.4992083013057709, average accuracy: 0.805625\n",
      "28th 100 batches, average loss: 0.49765098094940186, average accuracy: 0.808125\n",
      "29th 100 batches, average loss: 0.4715729057788849, average accuracy: 0.815625\n",
      "30th 100 batches, average loss: 0.5011380314826965, average accuracy: 0.809375\n",
      "31th 100 batches, average loss: 0.491812527179718, average accuracy: 0.816875\n",
      "32th 100 batches, average loss: 0.4603302776813507, average accuracy: 0.823125\n",
      "33th 100 batches, average loss: 0.4634690582752228, average accuracy: 0.8221875\n",
      "34th 100 batches, average loss: 0.44568413496017456, average accuracy: 0.8325\n",
      "35th 100 batches, average loss: 0.46856221556663513, average accuracy: 0.816875\n",
      "36th 100 batches, average loss: 0.44605591893196106, average accuracy: 0.82875\n",
      "37th 100 batches, average loss: 0.45835721492767334, average accuracy: 0.825625\n",
      "38th 100 batches, average loss: 0.4316718280315399, average accuracy: 0.834375\n",
      "39th 100 batches, average loss: 0.4671788513660431, average accuracy: 0.8171875\n",
      "40th 100 batches, average loss: 0.4603980779647827, average accuracy: 0.8215625\n",
      "41th 100 batches, average loss: 0.4148310422897339, average accuracy: 0.83875\n",
      "42th 100 batches, average loss: 0.4192495048046112, average accuracy: 0.8328125\n",
      "43th 100 batches, average loss: 0.43395599722862244, average accuracy: 0.8340625\n",
      "44th 100 batches, average loss: 0.4182319641113281, average accuracy: 0.8403125\n",
      "45th 100 batches, average loss: 0.4289442300796509, average accuracy: 0.83\n",
      "46th 100 batches, average loss: 0.43738940358161926, average accuracy: 0.8253125\n",
      "47th 100 batches, average loss: 0.41817864775657654, average accuracy: 0.84\n",
      "48th 100 batches, average loss: 0.38250789046287537, average accuracy: 0.85\n",
      "49th 100 batches, average loss: 0.404237300157547, average accuracy: 0.840625\n"
     ]
    }
   ],
   "source": [
    "train_model(lstm_cr, cr_loss, \"lstm_cr\", epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM with mean squared error Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMIdxer(\n",
      "  (embeddings): Embedding(11, 8)\n",
      "  (lstm): LSTM(8, 64, batch_first=True)\n",
      "  (hidden2idx): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_mse = LSTMIdxer(EMBEDDING_DIM, HIDDEN_DIM, 11, 1, BATCH_SIZE)\n",
    "mse_loss = nn.MSELoss()\n",
    "print(lstm_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.0872138738632202, average accuracy: 0.5240625\n",
      "1th 100 batches, average loss: 1.1440348625183105, average accuracy: 0.4890625\n",
      "2th 100 batches, average loss: 1.1651543378829956, average accuracy: 0.5115625\n",
      "3th 100 batches, average loss: 0.9776139259338379, average accuracy: 0.524375\n",
      "4th 100 batches, average loss: 0.9620973467826843, average accuracy: 0.555625\n",
      "5th 100 batches, average loss: 0.9998934864997864, average accuracy: 0.5465625\n",
      "6th 100 batches, average loss: 0.9990928173065186, average accuracy: 0.539375\n",
      "7th 100 batches, average loss: 0.975043773651123, average accuracy: 0.5496875\n",
      "8th 100 batches, average loss: 0.9823878407478333, average accuracy: 0.555\n",
      "9th 100 batches, average loss: 0.9894894361495972, average accuracy: 0.5721875\n",
      "10th 100 batches, average loss: 0.8785991668701172, average accuracy: 0.5784375\n",
      "11th 100 batches, average loss: 0.867185115814209, average accuracy: 0.599375\n",
      "12th 100 batches, average loss: 0.8007153272628784, average accuracy: 0.6128125\n",
      "13th 100 batches, average loss: 0.9697232842445374, average accuracy: 0.585\n",
      "14th 100 batches, average loss: 0.871918797492981, average accuracy: 0.5771875\n",
      "15th 100 batches, average loss: 0.926632285118103, average accuracy: 0.59625\n",
      "16th 100 batches, average loss: 0.8598223924636841, average accuracy: 0.59625\n",
      "17th 100 batches, average loss: 0.9023703932762146, average accuracy: 0.58875\n",
      "18th 100 batches, average loss: 0.8561033010482788, average accuracy: 0.58375\n",
      "19th 100 batches, average loss: 0.8661555647850037, average accuracy: 0.595625\n",
      "20th 100 batches, average loss: 0.8811770677566528, average accuracy: 0.5853125\n",
      "21th 100 batches, average loss: 0.8030621409416199, average accuracy: 0.61125\n",
      "22th 100 batches, average loss: 0.8823329210281372, average accuracy: 0.5971875\n",
      "23th 100 batches, average loss: 0.959190845489502, average accuracy: 0.5915625\n",
      "24th 100 batches, average loss: 0.7469570636749268, average accuracy: 0.62375\n",
      "25th 100 batches, average loss: 0.6660584211349487, average accuracy: 0.626875\n",
      "26th 100 batches, average loss: 0.7348197102546692, average accuracy: 0.6328125\n",
      "27th 100 batches, average loss: 0.7848840355873108, average accuracy: 0.625\n",
      "28th 100 batches, average loss: 0.8097758293151855, average accuracy: 0.6159375\n",
      "29th 100 batches, average loss: 0.7248161435127258, average accuracy: 0.621875\n",
      "30th 100 batches, average loss: 0.7485774159431458, average accuracy: 0.6278125\n",
      "31th 100 batches, average loss: 0.7834262251853943, average accuracy: 0.638125\n",
      "32th 100 batches, average loss: 0.7729957699775696, average accuracy: 0.613125\n",
      "33th 100 batches, average loss: 0.8507122993469238, average accuracy: 0.6046875\n",
      "34th 100 batches, average loss: 0.8318200707435608, average accuracy: 0.61625\n",
      "35th 100 batches, average loss: 0.7177577018737793, average accuracy: 0.633125\n",
      "36th 100 batches, average loss: 0.8289021253585815, average accuracy: 0.6153125\n",
      "37th 100 batches, average loss: 0.6939103007316589, average accuracy: 0.6584375\n",
      "38th 100 batches, average loss: 0.7906632423400879, average accuracy: 0.6190625\n",
      "39th 100 batches, average loss: 0.7439708113670349, average accuracy: 0.6171875\n",
      "40th 100 batches, average loss: 0.8035792708396912, average accuracy: 0.619375\n",
      "41th 100 batches, average loss: 0.7729546427726746, average accuracy: 0.60375\n",
      "42th 100 batches, average loss: 0.7399946451187134, average accuracy: 0.614375\n",
      "43th 100 batches, average loss: 0.8742782473564148, average accuracy: 0.5965625\n",
      "44th 100 batches, average loss: 0.6623466610908508, average accuracy: 0.6390625\n",
      "45th 100 batches, average loss: 0.7512964010238647, average accuracy: 0.638125\n",
      "46th 100 batches, average loss: 0.7076911330223083, average accuracy: 0.6315625\n",
      "47th 100 batches, average loss: 0.6992291808128357, average accuracy: 0.6471875\n",
      "48th 100 batches, average loss: 0.7476779818534851, average accuracy: 0.6425\n",
      "49th 100 batches, average loss: 0.7093114256858826, average accuracy: 0.628125\n"
     ]
    }
   ],
   "source": [
    "train_model(lstm_mse, mse_loss, \"lstm_mse\", epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "Predicted:\n",
      "None\n",
      "\n",
      "Actual:\n",
      "tensor([ 3,  4,  0,  4,  6,  0,  0,  0,  2,  8,  2,  0,  2,  1,\n",
      "         3,  0,  0,  3,  3,  2,  3,  0,  2,  2,  4,  2,  2,  2,\n",
      "         0,  3,  5,  1])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    predicted = lstm_cr(inputs)\n",
    "    print(\"Predicted:\\n{}\\n\\nActual:\\n{}\".format(predicted, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.2 Sequence to Sequence Network and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, seq, hidden):\n",
    "        embedded = self.embedding(seq)\n",
    "#         print(seq.size(), embedded.size(), hidden.size())\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "#         self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "#         self.attn = nn.Linear(self.hidden_size+1, self.max_length)\n",
    "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size+1, self.hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, di):\n",
    "#         embedded = self.embedding(input)\n",
    "#         embedded = self.dropout(embedded)\n",
    "#         print(input.size(), hidden.size())\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((input, hidden[0]), 1)), dim=1)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((encoder_outputs, hidden.transpose(0, 1).repeat(1, encoder_outputs.size(1), 1)), -1)), dim=-1).transpose(1, 2)\n",
    "#         print(attn_weights.size(), encoder_outputs.size())\n",
    "        attn_applied = torch.bmm(attn_weights,\n",
    "                                 encoder_outputs)\n",
    "#         print(attn_applied.size())\n",
    "        input = input.unsqueeze(1).float()\n",
    "#         print(input.size(), attn_applied[:, di, :].size())\n",
    "#         print(input, attn_applied[:, di, :])\n",
    "        output = torch.cat((input, attn_applied[:, di, :]), 1)\n",
    "#         print(output.size())\n",
    "        output = self.attn_combine(output).unsqueeze(1)\n",
    "#         print(output.size())\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print(output.size())\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "#         print(output.size())\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 0.25839762449264514, average accuracy: 0.1040625\n",
      "1th 100 batches, average loss: 0.1723626926812258, average accuracy: 0.3828125\n",
      "2th 100 batches, average loss: 0.16276010545817285, average accuracy: 0.4203125\n",
      "3th 100 batches, average loss: 0.15036865223537793, average accuracy: 0.473125\n",
      "4th 100 batches, average loss: 0.12807980076833203, average accuracy: 0.576875\n",
      "5th 100 batches, average loss: 0.1276687277988955, average accuracy: 0.5834375\n",
      "6th 100 batches, average loss: 0.12831343618306248, average accuracy: 0.580625\n",
      "7th 100 batches, average loss: 0.11694440717046911, average accuracy: 0.6440625\n",
      "8th 100 batches, average loss: 0.11716491756114097, average accuracy: 0.62875\n",
      "9th 100 batches, average loss: 0.11737928368828511, average accuracy: 0.631875\n",
      "10th 100 batches, average loss: 0.11287084183909675, average accuracy: 0.6475\n",
      "11th 100 batches, average loss: 0.10700342032042419, average accuracy: 0.6821875\n",
      "12th 100 batches, average loss: 0.11220546592365606, average accuracy: 0.6396875\n",
      "13th 100 batches, average loss: 0.10383611527356233, average accuracy: 0.69125\n",
      "14th 100 batches, average loss: 0.10326064554127781, average accuracy: 0.70875\n",
      "15th 100 batches, average loss: 0.10267075460065499, average accuracy: 0.698125\n",
      "16th 100 batches, average loss: 0.10597417966886002, average accuracy: 0.69125\n",
      "17th 100 batches, average loss: 0.1007754236459732, average accuracy: 0.7021875\n",
      "18th 100 batches, average loss: 0.10027246393940668, average accuracy: 0.7140625\n",
      "19th 100 batches, average loss: 0.10685290835120463, average accuracy: 0.6771875\n",
      "20th 100 batches, average loss: 0.09601674453778701, average accuracy: 0.7209375\n",
      "21th 100 batches, average loss: 0.0888847209648652, average accuracy: 0.74375\n",
      "22th 100 batches, average loss: 0.09873720250346445, average accuracy: 0.7134375\n",
      "23th 100 batches, average loss: 0.0882683132453398, average accuracy: 0.75625\n",
      "24th 100 batches, average loss: 0.08299516759135507, average accuracy: 0.779375\n",
      "25th 100 batches, average loss: 0.08532139799811618, average accuracy: 0.7575\n",
      "26th 100 batches, average loss: 0.09068530518900263, average accuracy: 0.7496875\n",
      "27th 100 batches, average loss: 0.09160891432653774, average accuracy: 0.7359375\n",
      "28th 100 batches, average loss: 0.0874755431847139, average accuracy: 0.7575\n",
      "29th 100 batches, average loss: 0.08834893185984007, average accuracy: 0.748125\n",
      "30th 100 batches, average loss: 0.09199941811236466, average accuracy: 0.7340625\n",
      "31th 100 batches, average loss: 0.0932804317636923, average accuracy: 0.7221875\n",
      "32th 100 batches, average loss: 0.0905380128188567, average accuracy: 0.7453125\n",
      "33th 100 batches, average loss: 0.08858424579555337, average accuracy: 0.75125\n",
      "34th 100 batches, average loss: 0.0876057044755329, average accuracy: 0.754375\n",
      "35th 100 batches, average loss: 0.08834924426945773, average accuracy: 0.7478125\n",
      "36th 100 batches, average loss: 0.08200530799952421, average accuracy: 0.7796875\n",
      "37th 100 batches, average loss: 0.09200585801493037, average accuracy: 0.7240625\n",
      "38th 100 batches, average loss: 0.08232306147163565, average accuracy: 0.7715625\n",
      "39th 100 batches, average loss: 0.08679161074486648, average accuracy: 0.744375\n",
      "40th 100 batches, average loss: 0.0863922502777793, average accuracy: 0.7509375\n",
      "41th 100 batches, average loss: 0.08373280694538898, average accuracy: 0.744375\n",
      "42th 100 batches, average loss: 0.08432488463141702, average accuracy: 0.7628125\n",
      "43th 100 batches, average loss: 0.07858003632588821, average accuracy: 0.7925\n",
      "44th 100 batches, average loss: 0.07983255026015368, average accuracy: 0.78375\n",
      "45th 100 batches, average loss: 0.07560419489036911, average accuracy: 0.7990625\n",
      "46th 100 batches, average loss: 0.0846477803046053, average accuracy: 0.7584375\n",
      "47th 100 batches, average loss: 0.08203015053814107, average accuracy: 0.7734375\n",
      "48th 100 batches, average loss: 0.08149331407113512, average accuracy: 0.7690625\n",
      "49th 100 batches, average loss: 0.07654577886516396, average accuracy: 0.7871875\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden(BATCH_SIZE)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # input_length = input_tensor.size(0)\n",
    "    input_length = 11\n",
    "    # target_length = target_tensor.size(0)\n",
    "    target_length = 11\n",
    "\n",
    "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    encoder_outputs, encoder_hidden = encoder(\n",
    "            input_tensor, encoder_hidden)\n",
    "#     for ei in range(input_length):\n",
    "#         encoder_output, encoder_hidden = encoder(\n",
    "#             input_tensor[:, ei], encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([0]*BATCH_SIZE, dtype=torch.long)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    current_correct = 0\n",
    "#     if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs, di)\n",
    "#             print(decoder_output.squeeze().size(), target_tensor[:, di].size())\n",
    "#             print(target_tensor)\n",
    "        loss += criterion(decoder_output.squeeze(), target_tensor[:, di])\n",
    "#         print(decoder_output.size())\n",
    "        topv, topi = decoder_output.topk(1, -1)\n",
    "#         print(topi)\n",
    "#         print(topi.size(), target_tensor.size())\n",
    "\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[:, di]  # Teacher forcing\n",
    "        else:\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "        \n",
    "        # find true positive samples for current position\n",
    "#         print(di)\n",
    "        topi = topi.squeeze().long()\n",
    "#         print(topi)\n",
    "        tp = topi + target_tensor[:, di] == 2\n",
    "#         print(target_tensor[:, di], tp)\n",
    "        current_correct += tp.sum()\n",
    "#         print(current_correct)\n",
    "#         if tp.sum() != 0:\n",
    "#             print(current_correct)\n",
    "#             print(\"!!!!!!!!!!\", tp.sum())\n",
    "            \n",
    "            \n",
    "#             return\n",
    "        \n",
    "#         if topi == 1 and target_tensor[di] == 1:\n",
    "            \n",
    "\n",
    "#     else:\n",
    "#         # Without teacher forcing: use its own predictions as the next input\n",
    "#         for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs, di)\n",
    "# #             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "#             loss += criterion(decoder_output.squeeze(), target_tensor[:, di])\n",
    "    \n",
    "            \n",
    "#         topv, topi = decoder_output.data.topk(1, -1)\n",
    "#         print(topv, topi)\n",
    "#         if topi == 1 and di\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "#     print(current_correct)\n",
    "    return (loss.item() / target_length, current_correct)\n",
    "\n",
    "def trainIters(encoder, decoder, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    # training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                   for i in range(n_iters)]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "            input_tensor, idx = data\n",
    "            target_tensor = torch.zeros(idx.size(0), 11, dtype=torch.long)\n",
    "            for j, v in enumerate(idx):\n",
    "                target_tensor[j] = oneHot(v)\n",
    "#             print(input_tensor.size())\n",
    "\n",
    "            loss, current_correct = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#             print(current_correct)\n",
    "            total_loss += loss\n",
    "            total_correct += current_correct\n",
    "#             print(total_correct)\n",
    "            total_sample += idx.size(0)\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct.item()/total_sample\n",
    "#                 print(total_correct, total_sample, avg_acc)\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "\n",
    "hidden_size = 32\n",
    "encoder1 = EncoderRNN(11, EMBEDDING_DIM, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, 2, dropout_p=0.1)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.7152857780456543, average accuracy: 0.3646875\n",
      "1th 100 batches, average loss: 1.2777975797653198, average accuracy: 0.491875\n",
      "2th 100 batches, average loss: 1.1254957914352417, average accuracy: 0.551875\n",
      "3th 100 batches, average loss: 1.0514452457427979, average accuracy: 0.5746875\n",
      "4th 100 batches, average loss: 0.9812842011451721, average accuracy: 0.6065625\n",
      "5th 100 batches, average loss: 0.9451970458030701, average accuracy: 0.62625\n",
      "6th 100 batches, average loss: 0.9167865514755249, average accuracy: 0.6471875\n",
      "7th 100 batches, average loss: 0.881909966468811, average accuracy: 0.6575\n",
      "8th 100 batches, average loss: 0.8493098616600037, average accuracy: 0.6721875\n",
      "9th 100 batches, average loss: 0.8256438970565796, average accuracy: 0.676875\n",
      "10th 100 batches, average loss: 0.7834854125976562, average accuracy: 0.6925\n",
      "11th 100 batches, average loss: 0.768293559551239, average accuracy: 0.69875\n",
      "12th 100 batches, average loss: 0.7726684808731079, average accuracy: 0.6984375\n",
      "13th 100 batches, average loss: 0.7636969685554504, average accuracy: 0.701875\n",
      "14th 100 batches, average loss: 0.7279285192489624, average accuracy: 0.7146875\n",
      "15th 100 batches, average loss: 0.6965036988258362, average accuracy: 0.728125\n",
      "16th 100 batches, average loss: 0.7351641654968262, average accuracy: 0.7184375\n",
      "17th 100 batches, average loss: 0.6685929298400879, average accuracy: 0.7446875\n",
      "18th 100 batches, average loss: 0.6933720111846924, average accuracy: 0.7371875\n",
      "19th 100 batches, average loss: 0.7067245244979858, average accuracy: 0.725625\n",
      "20th 100 batches, average loss: 0.6773123145103455, average accuracy: 0.736875\n",
      "21th 100 batches, average loss: 0.6371066570281982, average accuracy: 0.755\n",
      "22th 100 batches, average loss: 0.6276553869247437, average accuracy: 0.7590625\n",
      "23th 100 batches, average loss: 0.6520998477935791, average accuracy: 0.7475\n",
      "24th 100 batches, average loss: 0.6297122240066528, average accuracy: 0.7596875\n",
      "25th 100 batches, average loss: 0.6211199760437012, average accuracy: 0.7675\n",
      "26th 100 batches, average loss: 0.6260553598403931, average accuracy: 0.75375\n",
      "27th 100 batches, average loss: 0.6429333686828613, average accuracy: 0.759375\n",
      "28th 100 batches, average loss: 0.5921440720558167, average accuracy: 0.76875\n",
      "29th 100 batches, average loss: 0.6267905235290527, average accuracy: 0.7603125\n",
      "30th 100 batches, average loss: 0.5989559888839722, average accuracy: 0.7575\n",
      "31th 100 batches, average loss: 0.6013009548187256, average accuracy: 0.77375\n",
      "32th 100 batches, average loss: 0.5623913407325745, average accuracy: 0.7875\n",
      "33th 100 batches, average loss: 0.56758713722229, average accuracy: 0.785\n",
      "34th 100 batches, average loss: 0.5701059103012085, average accuracy: 0.785\n",
      "35th 100 batches, average loss: 0.5698592662811279, average accuracy: 0.7775\n",
      "36th 100 batches, average loss: 0.5916008949279785, average accuracy: 0.768125\n",
      "37th 100 batches, average loss: 0.5298674702644348, average accuracy: 0.7934375\n",
      "38th 100 batches, average loss: 0.5699432492256165, average accuracy: 0.775625\n",
      "39th 100 batches, average loss: 0.5609679818153381, average accuracy: 0.788125\n",
      "40th 100 batches, average loss: 0.5213664174079895, average accuracy: 0.7896875\n",
      "41th 100 batches, average loss: 0.5356726050376892, average accuracy: 0.79\n",
      "42th 100 batches, average loss: 0.5162120461463928, average accuracy: 0.7971875\n",
      "43th 100 batches, average loss: 0.5148076415061951, average accuracy: 0.79875\n",
      "44th 100 batches, average loss: 0.5139365196228027, average accuracy: 0.8084375\n",
      "45th 100 batches, average loss: 0.5037034749984741, average accuracy: 0.8090625\n",
      "46th 100 batches, average loss: 0.5184881687164307, average accuracy: 0.8034375\n",
      "47th 100 batches, average loss: 0.521589457988739, average accuracy: 0.80125\n",
      "48th 100 batches, average loss: 0.5112044811248779, average accuracy: 0.8065625\n",
      "49th 100 batches, average loss: 0.4969531297683716, average accuracy: 0.8090625\n"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN1(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length=10):\n",
    "        super(AttnDecoderRNN1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((encoder_outputs, hidden.transpose(0, 1).repeat(1, encoder_outputs.size(1), 1)), -1)), dim=-1).transpose(1, 2)\n",
    "        attn_applied = torch.bmm(attn_weights,\n",
    "                                 encoder_outputs)\n",
    "        output = F.relu(attn_applied)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "def trainIters1(encoder, decoder, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    # training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                   for i in range(n_iters)]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "            input_tensor, target_tensor = data\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden(BATCH_SIZE)\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            input_length = 11\n",
    "            target_length = 10\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(\n",
    "                    input_tensor, encoder_hidden)\n",
    "\n",
    "            decoder_input = torch.tensor([0]*BATCH_SIZE, dtype=torch.long)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            predicted = torch.argmax(decoder_output[:, -1, :], -1)\n",
    "            total_correct += (predicted == target_tensor).sum().item()\n",
    "            total_sample += target_tensor.size(0)\n",
    "            \n",
    "#             print(decoder_output.size(), target_tensor.size())\n",
    "            loss += criterion(decoder_output[:, -1, :], target_tensor)\n",
    "            total_loss += loss\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct/total_sample\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "                \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "    \n",
    "\n",
    "hidden_size = 32\n",
    "encoder1 = EncoderRNN(11, EMBEDDING_DIM, hidden_size)\n",
    "attn_decoder2 = AttnDecoderRNN1(hidden_size, 10)\n",
    "\n",
    "trainIters1(encoder1, attn_decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.9289747476577759, average accuracy: 0.3309375\n",
      "1th 100 batches, average loss: 1.5320793390274048, average accuracy: 0.43125\n",
      "2th 100 batches, average loss: 1.3366148471832275, average accuracy: 0.5021875\n",
      "3th 100 batches, average loss: 1.208532452583313, average accuracy: 0.5378125\n",
      "4th 100 batches, average loss: 1.1551700830459595, average accuracy: 0.5378125\n",
      "5th 100 batches, average loss: 1.1031537055969238, average accuracy: 0.5578125\n",
      "6th 100 batches, average loss: 1.0327820777893066, average accuracy: 0.590625\n",
      "7th 100 batches, average loss: 1.0193594694137573, average accuracy: 0.595\n",
      "8th 100 batches, average loss: 0.9857317209243774, average accuracy: 0.6278125\n",
      "9th 100 batches, average loss: 0.9393985271453857, average accuracy: 0.630625\n",
      "10th 100 batches, average loss: 0.9076352119445801, average accuracy: 0.6528125\n",
      "11th 100 batches, average loss: 0.8802374005317688, average accuracy: 0.65625\n",
      "12th 100 batches, average loss: 0.8838822245597839, average accuracy: 0.6625\n",
      "13th 100 batches, average loss: 0.9103472828865051, average accuracy: 0.6446875\n",
      "14th 100 batches, average loss: 0.8484116196632385, average accuracy: 0.6734375\n",
      "15th 100 batches, average loss: 0.8435828685760498, average accuracy: 0.668125\n",
      "16th 100 batches, average loss: 0.8150585889816284, average accuracy: 0.68375\n",
      "17th 100 batches, average loss: 0.8305620551109314, average accuracy: 0.67\n",
      "18th 100 batches, average loss: 0.8249624371528625, average accuracy: 0.6734375\n",
      "19th 100 batches, average loss: 0.8092833757400513, average accuracy: 0.6853125\n",
      "20th 100 batches, average loss: 0.7959918975830078, average accuracy: 0.6915625\n",
      "21th 100 batches, average loss: 0.7468760013580322, average accuracy: 0.70375\n",
      "22th 100 batches, average loss: 0.756644070148468, average accuracy: 0.706875\n",
      "23th 100 batches, average loss: 0.7223763465881348, average accuracy: 0.7146875\n",
      "24th 100 batches, average loss: 0.739158570766449, average accuracy: 0.71375\n",
      "25th 100 batches, average loss: 0.7440848350524902, average accuracy: 0.7071875\n",
      "26th 100 batches, average loss: 0.7104557156562805, average accuracy: 0.7246875\n",
      "27th 100 batches, average loss: 0.6945754289627075, average accuracy: 0.72875\n",
      "28th 100 batches, average loss: 0.6901281476020813, average accuracy: 0.7253125\n",
      "29th 100 batches, average loss: 0.6911598443984985, average accuracy: 0.7240625\n",
      "30th 100 batches, average loss: 0.678937554359436, average accuracy: 0.7365625\n",
      "31th 100 batches, average loss: 0.7182222008705139, average accuracy: 0.7221875\n",
      "32th 100 batches, average loss: 0.6728896498680115, average accuracy: 0.7284375\n",
      "33th 100 batches, average loss: 0.6347014307975769, average accuracy: 0.7496875\n",
      "34th 100 batches, average loss: 0.6670280694961548, average accuracy: 0.7284375\n",
      "35th 100 batches, average loss: 0.6723311543464661, average accuracy: 0.73875\n",
      "36th 100 batches, average loss: 0.6664590239524841, average accuracy: 0.7440625\n",
      "37th 100 batches, average loss: 0.6438427567481995, average accuracy: 0.7453125\n",
      "38th 100 batches, average loss: 0.6271318197250366, average accuracy: 0.75375\n",
      "39th 100 batches, average loss: 0.6202951669692993, average accuracy: 0.76\n",
      "40th 100 batches, average loss: 0.6113975644111633, average accuracy: 0.75625\n",
      "41th 100 batches, average loss: 0.6049038767814636, average accuracy: 0.76375\n",
      "42th 100 batches, average loss: 0.6150315403938293, average accuracy: 0.760625\n",
      "43th 100 batches, average loss: 0.6325927972793579, average accuracy: 0.7503125\n",
      "44th 100 batches, average loss: 0.6267486810684204, average accuracy: 0.7496875\n",
      "45th 100 batches, average loss: 0.6132122874259949, average accuracy: 0.7565625\n",
      "46th 100 batches, average loss: 0.5944593548774719, average accuracy: 0.7709375\n",
      "47th 100 batches, average loss: 0.604544460773468, average accuracy: 0.760625\n",
      "48th 100 batches, average loss: 0.557986319065094, average accuracy: 0.7821875\n",
      "49th 100 batches, average loss: 0.5994973182678223, average accuracy: 0.7653125\n"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN1(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length=10):\n",
    "        super(AttnDecoderRNN1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((encoder_outputs, hidden.transpose(0, 1).repeat(1, encoder_outputs.size(1), 1)), -1)), dim=-1).transpose(1, 2)\n",
    "        attn_applied = torch.bmm(attn_weights,\n",
    "                                 encoder_outputs)\n",
    "#         output = F.relu(attn_applied)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "def trainIters1(encoder, decoder, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "            input_tensor, target_tensor = data\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden(BATCH_SIZE)\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            input_length = 11\n",
    "            target_length = 10\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(\n",
    "                    input_tensor, encoder_hidden)\n",
    "\n",
    "            decoder_hidden = decoder.initHidden(BATCH_SIZE)  # Modified\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            predicted = torch.argmax(decoder_output[:, -1, :], -1)\n",
    "            total_correct += (predicted == target_tensor).sum().item()\n",
    "            total_sample += target_tensor.size(0)\n",
    "            \n",
    "#             print(decoder_output.size(), target_tensor.size())\n",
    "            loss += criterion(decoder_output[:, -1, :], target_tensor)\n",
    "            total_loss += loss\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct/total_sample\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "                \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "hidden_size = 32\n",
    "encoder1 = EncoderRNN(11, EMBEDDING_DIM, hidden_size)\n",
    "attn_decoder2 = AttnDecoderRNN1(hidden_size, 10)\n",
    "\n",
    "trainIters1(encoder1, attn_decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.6825774908065796, average accuracy: 0.3715625\n",
      "1th 100 batches, average loss: 1.3043633699417114, average accuracy: 0.46625\n",
      "2th 100 batches, average loss: 1.1780871152877808, average accuracy: 0.5334375\n",
      "3th 100 batches, average loss: 1.0982396602630615, average accuracy: 0.5675\n",
      "4th 100 batches, average loss: 1.0089439153671265, average accuracy: 0.61125\n",
      "5th 100 batches, average loss: 0.9695562124252319, average accuracy: 0.6215625\n",
      "6th 100 batches, average loss: 0.9329012036323547, average accuracy: 0.635\n",
      "7th 100 batches, average loss: 0.8980787396430969, average accuracy: 0.6578125\n",
      "8th 100 batches, average loss: 0.8861342072486877, average accuracy: 0.6503125\n",
      "9th 100 batches, average loss: 0.8497655987739563, average accuracy: 0.6675\n",
      "10th 100 batches, average loss: 0.8327069878578186, average accuracy: 0.6753125\n",
      "11th 100 batches, average loss: 0.7890624403953552, average accuracy: 0.6896875\n",
      "12th 100 batches, average loss: 0.7894924283027649, average accuracy: 0.6878125\n",
      "13th 100 batches, average loss: 0.7734845876693726, average accuracy: 0.698125\n",
      "14th 100 batches, average loss: 0.735942006111145, average accuracy: 0.7175\n",
      "15th 100 batches, average loss: 0.7660290002822876, average accuracy: 0.6984375\n",
      "16th 100 batches, average loss: 0.6984676122665405, average accuracy: 0.728125\n",
      "17th 100 batches, average loss: 0.701306164264679, average accuracy: 0.731875\n",
      "18th 100 batches, average loss: 0.6854255795478821, average accuracy: 0.7246875\n",
      "19th 100 batches, average loss: 0.6854826211929321, average accuracy: 0.740625\n",
      "20th 100 batches, average loss: 0.6513946056365967, average accuracy: 0.7465625\n",
      "21th 100 batches, average loss: 0.666221022605896, average accuracy: 0.7453125\n",
      "22th 100 batches, average loss: 0.6468265056610107, average accuracy: 0.7375\n",
      "23th 100 batches, average loss: 0.6687754988670349, average accuracy: 0.7396875\n",
      "24th 100 batches, average loss: 0.6280570030212402, average accuracy: 0.7503125\n",
      "25th 100 batches, average loss: 0.6332505941390991, average accuracy: 0.751875\n",
      "26th 100 batches, average loss: 0.610517144203186, average accuracy: 0.7615625\n",
      "27th 100 batches, average loss: 0.6265583634376526, average accuracy: 0.7640625\n",
      "28th 100 batches, average loss: 0.6266496181488037, average accuracy: 0.7575\n",
      "29th 100 batches, average loss: 0.5897421836853027, average accuracy: 0.7703125\n",
      "30th 100 batches, average loss: 0.589455783367157, average accuracy: 0.7609375\n",
      "31th 100 batches, average loss: 0.585477352142334, average accuracy: 0.7703125\n",
      "32th 100 batches, average loss: 0.6065717935562134, average accuracy: 0.765\n",
      "33th 100 batches, average loss: 0.5857353806495667, average accuracy: 0.7703125\n",
      "34th 100 batches, average loss: 0.5697181820869446, average accuracy: 0.7725\n",
      "35th 100 batches, average loss: 0.5721840858459473, average accuracy: 0.773125\n",
      "36th 100 batches, average loss: 0.5712863206863403, average accuracy: 0.7771875\n",
      "37th 100 batches, average loss: 0.5856386423110962, average accuracy: 0.76375\n",
      "38th 100 batches, average loss: 0.5799967646598816, average accuracy: 0.7725\n",
      "39th 100 batches, average loss: 0.5301649570465088, average accuracy: 0.7915625\n",
      "40th 100 batches, average loss: 0.5614017844200134, average accuracy: 0.7840625\n",
      "41th 100 batches, average loss: 0.5613216161727905, average accuracy: 0.7821875\n",
      "42th 100 batches, average loss: 0.5330560207366943, average accuracy: 0.7846875\n",
      "43th 100 batches, average loss: 0.5188431143760681, average accuracy: 0.801875\n",
      "44th 100 batches, average loss: 0.5244265794754028, average accuracy: 0.800625\n",
      "45th 100 batches, average loss: 0.5307453274726868, average accuracy: 0.79625\n",
      "46th 100 batches, average loss: 0.5314562916755676, average accuracy: 0.7821875\n",
      "47th 100 batches, average loss: 0.5150488615036011, average accuracy: 0.8003125\n",
      "48th 100 batches, average loss: 0.5155620574951172, average accuracy: 0.79\n",
      "49th 100 batches, average loss: 0.5090755820274353, average accuracy: 0.8075\n"
     ]
    }
   ],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, idx_size, batch_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to index space\n",
    "        self.hidden2idx = nn.Linear(hidden_dim, idx_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, seq, hidden):\n",
    "        embeds = self.embeddings(seq)\n",
    "        out, hidden = self.lstm(embeds, hidden) \n",
    "        return out, hidden[0]\n",
    "\n",
    "class AttnDecoderRNN2(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length=10):\n",
    "        super(AttnDecoderRNN2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden_concat = torch.cat((encoder_outputs, hidden.transpose(0, 1).repeat(1, encoder_outputs.size(1), 1)), -1)\n",
    "        attn_weights = F.softmax(self.attn(hidden_concat), dim=-1).transpose(1, 2)\n",
    "        attn_applied = torch.bmm(attn_weights,\n",
    "                                 encoder_outputs)\n",
    "        output = F.relu(attn_applied)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "def trainIters1(encoder, decoder, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "            input_tensor, target_tensor = data\n",
    "            \n",
    "            encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            input_length = 11\n",
    "            target_length = 10\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(\n",
    "                    input_tensor, encoder_hidden)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            predicted = torch.argmax(decoder_output[:, -1, :], -1)\n",
    "            total_correct += (predicted == target_tensor).sum().item()\n",
    "            total_sample += target_tensor.size(0)\n",
    "            \n",
    "            loss += criterion(decoder_output[:, -1, :], target_tensor)\n",
    "            total_loss += loss\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct/total_sample\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "                \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "hidden_size = 32\n",
    "encoder2 = EncoderLSTM(EMBEDDING_DIM, hidden_size, 11, 10, BATCH_SIZE)\n",
    "attn_decoder2 = AttnDecoderRNN2(hidden_size, 10)\n",
    "\n",
    "trainIters1(encoder2, attn_decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.9334077835083008, average accuracy: 0.3490625\n",
      "1th 100 batches, average loss: 1.4360328912734985, average accuracy: 0.448125\n",
      "2th 100 batches, average loss: 1.1527937650680542, average accuracy: 0.545\n",
      "3th 100 batches, average loss: 1.0687216520309448, average accuracy: 0.584375\n",
      "4th 100 batches, average loss: 0.9297465682029724, average accuracy: 0.6390625\n",
      "5th 100 batches, average loss: 0.8858238458633423, average accuracy: 0.6665625\n",
      "6th 100 batches, average loss: 0.8168993592262268, average accuracy: 0.67375\n",
      "7th 100 batches, average loss: 0.769267201423645, average accuracy: 0.7065625\n",
      "8th 100 batches, average loss: 0.7238100171089172, average accuracy: 0.7203125\n",
      "9th 100 batches, average loss: 0.7250379323959351, average accuracy: 0.7303125\n",
      "10th 100 batches, average loss: 0.670809268951416, average accuracy: 0.7525\n",
      "11th 100 batches, average loss: 0.611447274684906, average accuracy: 0.761875\n",
      "12th 100 batches, average loss: 0.633238673210144, average accuracy: 0.7584375\n",
      "13th 100 batches, average loss: 0.6112405061721802, average accuracy: 0.7640625\n",
      "14th 100 batches, average loss: 0.6145076155662537, average accuracy: 0.7609375\n",
      "15th 100 batches, average loss: 0.5891254544258118, average accuracy: 0.774375\n",
      "16th 100 batches, average loss: 0.539106547832489, average accuracy: 0.790625\n",
      "17th 100 batches, average loss: 0.5164226293563843, average accuracy: 0.8028125\n",
      "18th 100 batches, average loss: 0.5227810144424438, average accuracy: 0.7953125\n",
      "19th 100 batches, average loss: 0.5224928259849548, average accuracy: 0.8040625\n",
      "20th 100 batches, average loss: 0.5195874571800232, average accuracy: 0.7978125\n",
      "21th 100 batches, average loss: 0.49322688579559326, average accuracy: 0.810625\n",
      "22th 100 batches, average loss: 0.5117356777191162, average accuracy: 0.79375\n",
      "23th 100 batches, average loss: 0.47289833426475525, average accuracy: 0.8215625\n",
      "24th 100 batches, average loss: 0.4860531985759735, average accuracy: 0.8153125\n",
      "25th 100 batches, average loss: 0.4461589455604553, average accuracy: 0.8315625\n",
      "26th 100 batches, average loss: 0.4550943374633789, average accuracy: 0.8178125\n",
      "27th 100 batches, average loss: 0.44488468766212463, average accuracy: 0.8315625\n",
      "28th 100 batches, average loss: 0.4563252925872803, average accuracy: 0.8215625\n",
      "29th 100 batches, average loss: 0.4320671856403351, average accuracy: 0.8384375\n",
      "30th 100 batches, average loss: 0.40810897946357727, average accuracy: 0.843125\n",
      "31th 100 batches, average loss: 0.4114171862602234, average accuracy: 0.84875\n",
      "32th 100 batches, average loss: 0.40818047523498535, average accuracy: 0.8459375\n",
      "33th 100 batches, average loss: 0.41563189029693604, average accuracy: 0.845625\n",
      "34th 100 batches, average loss: 0.3974638879299164, average accuracy: 0.8509375\n",
      "35th 100 batches, average loss: 0.400948703289032, average accuracy: 0.8459375\n",
      "36th 100 batches, average loss: 0.38026168942451477, average accuracy: 0.8528125\n",
      "37th 100 batches, average loss: 0.3822164535522461, average accuracy: 0.8540625\n",
      "38th 100 batches, average loss: 0.36591029167175293, average accuracy: 0.8584375\n",
      "39th 100 batches, average loss: 0.3845710754394531, average accuracy: 0.8515625\n",
      "40th 100 batches, average loss: 0.3878529667854309, average accuracy: 0.8503125\n",
      "41th 100 batches, average loss: 0.37125319242477417, average accuracy: 0.8575\n",
      "42th 100 batches, average loss: 0.36299169063568115, average accuracy: 0.86\n",
      "43th 100 batches, average loss: 0.373343825340271, average accuracy: 0.8540625\n",
      "44th 100 batches, average loss: 0.33325430750846863, average accuracy: 0.868125\n",
      "45th 100 batches, average loss: 0.3484582006931305, average accuracy: 0.865\n",
      "46th 100 batches, average loss: 0.3733643591403961, average accuracy: 0.855\n",
      "47th 100 batches, average loss: 0.3164637088775635, average accuracy: 0.87875\n",
      "48th 100 batches, average loss: 0.33364394307136536, average accuracy: 0.8653125\n",
      "49th 100 batches, average loss: 0.34206897020339966, average accuracy: 0.8753125\n"
     ]
    }
   ],
   "source": [
    "class EncoderLSTM1(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, idx_size, batch_size):\n",
    "        super(EncoderLSTM1, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to index space\n",
    "        self.hidden2idx = nn.Linear(hidden_dim, idx_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, seq, hidden):\n",
    "        embeds = self.embeddings(seq)\n",
    "        out, hidden = self.lstm(embeds, hidden) \n",
    "        return out, hidden\n",
    "\n",
    "class AttnDecoderRNN3(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length=10):\n",
    "        super(AttnDecoderRNN3, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden_concat = torch.cat((encoder_outputs, hidden[0].transpose(0, 1).repeat(1, encoder_outputs.size(1), 1)), -1)\n",
    "        attn_weights = F.softmax(self.attn(hidden_concat), dim=-1).transpose(1, 2)\n",
    "        attn_applied = torch.bmm(attn_weights,\n",
    "                                 encoder_outputs)\n",
    "        output = F.relu(attn_applied)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        return output, hidden[0], attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "def trainIters1(encoder, decoder, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "            input_tensor, target_tensor = data\n",
    "            \n",
    "            encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            input_length = 11\n",
    "            target_length = 10\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(\n",
    "                    input_tensor, encoder_hidden)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            predicted = torch.argmax(decoder_output[:, -1, :], -1)\n",
    "            total_correct += (predicted == target_tensor).sum().item()\n",
    "            total_sample += target_tensor.size(0)\n",
    "            \n",
    "            loss += criterion(decoder_output[:, -1, :], target_tensor)\n",
    "            total_loss += loss\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct/total_sample\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "                \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "hidden_size = 128\n",
    "encoder3 = EncoderLSTM1(EMBEDDING_DIM, hidden_size, 11, 10, BATCH_SIZE)\n",
    "attn_decoder3 = AttnDecoderRNN3(hidden_size, 10)\n",
    "\n",
    "trainIters1(encoder3, attn_decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.8035305738449097, average accuracy: 0.3596875\n",
      "1th 100 batches, average loss: 1.4236353635787964, average accuracy: 0.4425\n",
      "2th 100 batches, average loss: 1.2316842079162598, average accuracy: 0.5128125\n",
      "3th 100 batches, average loss: 1.0613094568252563, average accuracy: 0.5803125\n",
      "4th 100 batches, average loss: 0.9940977692604065, average accuracy: 0.6040625\n",
      "5th 100 batches, average loss: 0.8811402916908264, average accuracy: 0.66375\n",
      "6th 100 batches, average loss: 0.8457968831062317, average accuracy: 0.6790625\n",
      "7th 100 batches, average loss: 0.7843658328056335, average accuracy: 0.701875\n",
      "8th 100 batches, average loss: 0.7652029991149902, average accuracy: 0.7090625\n",
      "9th 100 batches, average loss: 0.709677517414093, average accuracy: 0.731875\n",
      "10th 100 batches, average loss: 0.6592293381690979, average accuracy: 0.74625\n",
      "11th 100 batches, average loss: 0.6432557702064514, average accuracy: 0.7484375\n",
      "12th 100 batches, average loss: 0.6190547347068787, average accuracy: 0.761875\n",
      "13th 100 batches, average loss: 0.61346834897995, average accuracy: 0.76375\n",
      "14th 100 batches, average loss: 0.5810326337814331, average accuracy: 0.780625\n",
      "15th 100 batches, average loss: 0.5905209183692932, average accuracy: 0.7709375\n",
      "16th 100 batches, average loss: 0.5731008052825928, average accuracy: 0.779375\n",
      "17th 100 batches, average loss: 0.5269200801849365, average accuracy: 0.8040625\n",
      "18th 100 batches, average loss: 0.5113649964332581, average accuracy: 0.7996875\n",
      "19th 100 batches, average loss: 0.5175437331199646, average accuracy: 0.7975\n",
      "20th 100 batches, average loss: 0.497731477022171, average accuracy: 0.81125\n",
      "21th 100 batches, average loss: 0.48121529817581177, average accuracy: 0.8128125\n",
      "22th 100 batches, average loss: 0.47496458888053894, average accuracy: 0.82125\n",
      "23th 100 batches, average loss: 0.48231858015060425, average accuracy: 0.8153125\n",
      "24th 100 batches, average loss: 0.46057602763175964, average accuracy: 0.8203125\n",
      "25th 100 batches, average loss: 0.44729387760162354, average accuracy: 0.8278125\n",
      "26th 100 batches, average loss: 0.4573700726032257, average accuracy: 0.8140625\n",
      "27th 100 batches, average loss: 0.44960540533065796, average accuracy: 0.8284375\n",
      "28th 100 batches, average loss: 0.43414297699928284, average accuracy: 0.83125\n",
      "29th 100 batches, average loss: 0.44839903712272644, average accuracy: 0.82875\n",
      "30th 100 batches, average loss: 0.4444267153739929, average accuracy: 0.8265625\n",
      "31th 100 batches, average loss: 0.4262782037258148, average accuracy: 0.835625\n",
      "32th 100 batches, average loss: 0.4276109039783478, average accuracy: 0.834375\n",
      "33th 100 batches, average loss: 0.41582202911376953, average accuracy: 0.83375\n",
      "34th 100 batches, average loss: 0.3993566632270813, average accuracy: 0.8471875\n",
      "35th 100 batches, average loss: 0.41925498843193054, average accuracy: 0.83625\n",
      "36th 100 batches, average loss: 0.4093471169471741, average accuracy: 0.849375\n",
      "37th 100 batches, average loss: 0.3986576795578003, average accuracy: 0.8471875\n",
      "38th 100 batches, average loss: 0.39086297154426575, average accuracy: 0.8525\n",
      "39th 100 batches, average loss: 0.3681919574737549, average accuracy: 0.8625\n",
      "40th 100 batches, average loss: 0.3847846984863281, average accuracy: 0.8503125\n",
      "41th 100 batches, average loss: 0.3791102170944214, average accuracy: 0.8515625\n",
      "42th 100 batches, average loss: 0.37778544425964355, average accuracy: 0.851875\n",
      "43th 100 batches, average loss: 0.35559502243995667, average accuracy: 0.8640625\n",
      "44th 100 batches, average loss: 0.3444404602050781, average accuracy: 0.86375\n",
      "45th 100 batches, average loss: 0.3293067216873169, average accuracy: 0.8703125\n",
      "46th 100 batches, average loss: 0.3418089747428894, average accuracy: 0.870625\n",
      "47th 100 batches, average loss: 0.3718479871749878, average accuracy: 0.85875\n",
      "48th 100 batches, average loss: 0.33960092067718506, average accuracy: 0.8634375\n",
      "49th 100 batches, average loss: 0.3206869959831238, average accuracy: 0.883125\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder3 = EncoderLSTM1(EMBEDDING_DIM, hidden_size, 11, 10, BATCH_SIZE)\n",
    "attn_decoder3 = AttnDecoderRNN3(hidden_size, 10)\n",
    "\n",
    "trainIters1(encoder3, attn_decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.9609668254852295, average accuracy: 0.3334375\n",
      "1th 100 batches, average loss: 1.5360926389694214, average accuracy: 0.4009375\n",
      "2th 100 batches, average loss: 1.2768477201461792, average accuracy: 0.48875\n",
      "3th 100 batches, average loss: 1.1215893030166626, average accuracy: 0.561875\n",
      "4th 100 batches, average loss: 0.9726362824440002, average accuracy: 0.62625\n",
      "5th 100 batches, average loss: 0.9060349464416504, average accuracy: 0.64875\n",
      "6th 100 batches, average loss: 0.84651780128479, average accuracy: 0.67125\n",
      "7th 100 batches, average loss: 0.8034326434135437, average accuracy: 0.6853125\n",
      "8th 100 batches, average loss: 0.7657501697540283, average accuracy: 0.7040625\n",
      "9th 100 batches, average loss: 0.7156890034675598, average accuracy: 0.724375\n",
      "10th 100 batches, average loss: 0.7101845741271973, average accuracy: 0.735625\n",
      "11th 100 batches, average loss: 0.6642599701881409, average accuracy: 0.7503125\n",
      "12th 100 batches, average loss: 0.6326753497123718, average accuracy: 0.7765625\n",
      "13th 100 batches, average loss: 0.6105777621269226, average accuracy: 0.764375\n",
      "14th 100 batches, average loss: 0.5961763262748718, average accuracy: 0.768125\n",
      "15th 100 batches, average loss: 0.5686975121498108, average accuracy: 0.7871875\n",
      "16th 100 batches, average loss: 0.5952042937278748, average accuracy: 0.7803125\n",
      "17th 100 batches, average loss: 0.5596888065338135, average accuracy: 0.785625\n",
      "18th 100 batches, average loss: 0.5475689172744751, average accuracy: 0.788125\n",
      "19th 100 batches, average loss: 0.5259871482849121, average accuracy: 0.7984375\n",
      "20th 100 batches, average loss: 0.5248042941093445, average accuracy: 0.7996875\n",
      "21th 100 batches, average loss: 0.4964175820350647, average accuracy: 0.8090625\n",
      "22th 100 batches, average loss: 0.5012328028678894, average accuracy: 0.8021875\n",
      "23th 100 batches, average loss: 0.48286762833595276, average accuracy: 0.8259375\n",
      "24th 100 batches, average loss: 0.46915003657341003, average accuracy: 0.818125\n",
      "25th 100 batches, average loss: 0.48311129212379456, average accuracy: 0.81625\n",
      "26th 100 batches, average loss: 0.48162761330604553, average accuracy: 0.8178125\n",
      "27th 100 batches, average loss: 0.4597357213497162, average accuracy: 0.8259375\n",
      "28th 100 batches, average loss: 0.4326593279838562, average accuracy: 0.83375\n",
      "29th 100 batches, average loss: 0.44330623745918274, average accuracy: 0.834375\n",
      "30th 100 batches, average loss: 0.4763014614582062, average accuracy: 0.8215625\n",
      "31th 100 batches, average loss: 0.44640570878982544, average accuracy: 0.8353125\n",
      "32th 100 batches, average loss: 0.42261818051338196, average accuracy: 0.8365625\n",
      "33th 100 batches, average loss: 0.4142167270183563, average accuracy: 0.8453125\n",
      "34th 100 batches, average loss: 0.41304728388786316, average accuracy: 0.8384375\n",
      "35th 100 batches, average loss: 0.42228397727012634, average accuracy: 0.836875\n",
      "36th 100 batches, average loss: 0.4134823977947235, average accuracy: 0.836875\n",
      "37th 100 batches, average loss: 0.40484824776649475, average accuracy: 0.843125\n",
      "38th 100 batches, average loss: 0.3928780257701874, average accuracy: 0.856875\n",
      "39th 100 batches, average loss: 0.39958712458610535, average accuracy: 0.84375\n",
      "40th 100 batches, average loss: 0.4079136252403259, average accuracy: 0.8359375\n",
      "41th 100 batches, average loss: 0.4017123878002167, average accuracy: 0.849375\n",
      "42th 100 batches, average loss: 0.3604421615600586, average accuracy: 0.8571875\n",
      "43th 100 batches, average loss: 0.40464064478874207, average accuracy: 0.8396875\n",
      "44th 100 batches, average loss: 0.41826069355010986, average accuracy: 0.8428125\n",
      "45th 100 batches, average loss: 0.3622034192085266, average accuracy: 0.8609375\n",
      "46th 100 batches, average loss: 0.39728298783302307, average accuracy: 0.848125\n",
      "47th 100 batches, average loss: 0.35947781801223755, average accuracy: 0.8634375\n",
      "48th 100 batches, average loss: 0.37062057852745056, average accuracy: 0.8646875\n",
      "49th 100 batches, average loss: 0.36267295479774475, average accuracy: 0.8603125\n"
     ]
    }
   ],
   "source": [
    "class EncoderLSTM1(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, idx_size, batch_size, num_layers):\n",
    "        super(EncoderLSTM1, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to index space\n",
    "        self.hidden2idx = nn.Linear(hidden_dim, idx_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, seq, hidden):\n",
    "        embeds = self.embeddings(seq)\n",
    "        out, hidden = self.lstm(embeds, hidden) \n",
    "        return out, hidden\n",
    "\n",
    "class AttnDecoderRNN3(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length=10):\n",
    "        super(AttnDecoderRNN3, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden_concat = torch.cat((encoder_outputs, hidden[0].transpose(0, 1).repeat(1, encoder_outputs.size(1), 1)), -1)\n",
    "        attn_weights = F.softmax(self.attn(hidden_concat), dim=-1).transpose(1, 2)\n",
    "        attn_applied = torch.bmm(attn_weights,\n",
    "                                 encoder_outputs)\n",
    "        output = F.relu(attn_applied)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        return output, hidden[0], attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "def trainIters1(encoder, decoder, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "            input_tensor, target_tensor = data\n",
    "            \n",
    "            encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            input_length = 11\n",
    "            target_length = 10\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(\n",
    "                    input_tensor, encoder_hidden)\n",
    "\n",
    "            decoder_hidden = [s[1].unsqueeze(0) for s in encoder_hidden]\n",
    "#             print(decoder_hidden[0].size(), decoder_hidden[1].size())\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            predicted = torch.argmax(decoder_output[:, -1, :], -1)\n",
    "            total_correct += (predicted == target_tensor).sum().item()\n",
    "            total_sample += target_tensor.size(0)\n",
    "            \n",
    "            loss += criterion(decoder_output[:, -1, :], target_tensor)\n",
    "            total_loss += loss\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct/total_sample\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "                \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "hidden_size = 128\n",
    "encoder3 = EncoderLSTM1(EMBEDDING_DIM, hidden_size, 11, 10, BATCH_SIZE, 2)\n",
    "attn_decoder3 = AttnDecoderRNN3(hidden_size, 10)\n",
    "\n",
    "trainIters1(encoder3, attn_decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.8175301551818848, average accuracy: 0.3578125\n",
      "1th 100 batches, average loss: 1.4232324361801147, average accuracy: 0.4371875\n",
      "2th 100 batches, average loss: 1.257949948310852, average accuracy: 0.5065625\n",
      "3th 100 batches, average loss: 1.116391658782959, average accuracy: 0.55625\n",
      "4th 100 batches, average loss: 0.9897517561912537, average accuracy: 0.630625\n",
      "5th 100 batches, average loss: 0.9154134392738342, average accuracy: 0.658125\n",
      "6th 100 batches, average loss: 0.8813705444335938, average accuracy: 0.66125\n",
      "7th 100 batches, average loss: 0.850842297077179, average accuracy: 0.6790625\n",
      "8th 100 batches, average loss: 0.7693002223968506, average accuracy: 0.7096875\n",
      "9th 100 batches, average loss: 0.7447303533554077, average accuracy: 0.72125\n",
      "10th 100 batches, average loss: 0.702324628829956, average accuracy: 0.7275\n",
      "11th 100 batches, average loss: 0.6373677849769592, average accuracy: 0.761875\n",
      "12th 100 batches, average loss: 0.6514743566513062, average accuracy: 0.750625\n",
      "13th 100 batches, average loss: 0.6100313067436218, average accuracy: 0.7765625\n",
      "14th 100 batches, average loss: 0.59778892993927, average accuracy: 0.77125\n",
      "15th 100 batches, average loss: 0.5624030232429504, average accuracy: 0.7825\n",
      "16th 100 batches, average loss: 0.5187747478485107, average accuracy: 0.803125\n",
      "17th 100 batches, average loss: 0.5394580960273743, average accuracy: 0.7953125\n",
      "18th 100 batches, average loss: 0.5281445980072021, average accuracy: 0.80125\n",
      "19th 100 batches, average loss: 0.5007989406585693, average accuracy: 0.814375\n",
      "20th 100 batches, average loss: 0.5064216256141663, average accuracy: 0.8159375\n",
      "21th 100 batches, average loss: 0.47666627168655396, average accuracy: 0.8240625\n",
      "22th 100 batches, average loss: 0.4802757203578949, average accuracy: 0.8215625\n",
      "23th 100 batches, average loss: 0.45203059911727905, average accuracy: 0.8284375\n",
      "24th 100 batches, average loss: 0.47564542293548584, average accuracy: 0.8175\n",
      "25th 100 batches, average loss: 0.4766671359539032, average accuracy: 0.8140625\n",
      "26th 100 batches, average loss: 0.43214210867881775, average accuracy: 0.8378125\n",
      "27th 100 batches, average loss: 0.4379763901233673, average accuracy: 0.8340625\n",
      "28th 100 batches, average loss: 0.4309903681278229, average accuracy: 0.8375\n",
      "29th 100 batches, average loss: 0.42344626784324646, average accuracy: 0.840625\n",
      "30th 100 batches, average loss: 0.42743074893951416, average accuracy: 0.8396875\n",
      "31th 100 batches, average loss: 0.39802631735801697, average accuracy: 0.8490625\n",
      "32th 100 batches, average loss: 0.42726975679397583, average accuracy: 0.8303125\n",
      "33th 100 batches, average loss: 0.41604188084602356, average accuracy: 0.8409375\n",
      "34th 100 batches, average loss: 0.4036616086959839, average accuracy: 0.8490625\n",
      "35th 100 batches, average loss: 0.35694751143455505, average accuracy: 0.865\n",
      "36th 100 batches, average loss: 0.35756397247314453, average accuracy: 0.863125\n",
      "37th 100 batches, average loss: 0.3882879316806793, average accuracy: 0.8528125\n",
      "38th 100 batches, average loss: 0.39292052388191223, average accuracy: 0.848125\n",
      "39th 100 batches, average loss: 0.3581172227859497, average accuracy: 0.8559375\n",
      "40th 100 batches, average loss: 0.3376166820526123, average accuracy: 0.87\n",
      "41th 100 batches, average loss: 0.33197271823883057, average accuracy: 0.8690625\n",
      "42th 100 batches, average loss: 0.3522656261920929, average accuracy: 0.86375\n",
      "43th 100 batches, average loss: 0.33654728531837463, average accuracy: 0.87375\n",
      "44th 100 batches, average loss: 0.33256712555885315, average accuracy: 0.873125\n",
      "45th 100 batches, average loss: 0.35660210251808167, average accuracy: 0.865\n",
      "46th 100 batches, average loss: 0.30970704555511475, average accuracy: 0.879375\n",
      "47th 100 batches, average loss: 0.3340684771537781, average accuracy: 0.8746875\n",
      "48th 100 batches, average loss: 0.31560105085372925, average accuracy: 0.8809375\n",
      "49th 100 batches, average loss: 0.31741803884506226, average accuracy: 0.879375\n"
     ]
    }
   ],
   "source": [
    "class EncoderLSTM1(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, idx_size, batch_size, num_layers):\n",
    "        super(EncoderLSTM1, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to index space\n",
    "        self.hidden2idx = nn.Linear(hidden_dim, idx_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, seq, hidden):\n",
    "        embeds = self.embeddings(seq)\n",
    "        out, hidden = self.lstm(embeds, hidden) \n",
    "        return out, hidden\n",
    "\n",
    "class AttnDecoderRNN3(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, max_length=10):\n",
    "        super(AttnDecoderRNN3, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden_concat = torch.cat((encoder_outputs, hidden[0][-1].unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)), -1)\n",
    "        attn_weights = F.softmax(self.attn(hidden_concat), dim=-1).transpose(1, 2)\n",
    "        attn_applied = torch.bmm(attn_weights,\n",
    "                                 encoder_outputs)\n",
    "        output = F.relu(attn_applied)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        return output, hidden[0], attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "def trainIters1(encoder, decoder, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "            input_tensor, target_tensor = data\n",
    "            \n",
    "            encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            input_length = 11\n",
    "            target_length = 10\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(\n",
    "                    input_tensor, encoder_hidden)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "#             print(decoder_hidden[0].size(), decoder_hidden[1].size())\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            predicted = torch.argmax(decoder_output[:, -1, :], -1)\n",
    "            total_correct += (predicted == target_tensor).sum().item()\n",
    "            total_sample += target_tensor.size(0)\n",
    "            \n",
    "            loss += criterion(decoder_output[:, -1, :], target_tensor)\n",
    "            total_loss += loss\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct/total_sample\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "                \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "hidden_size = 128\n",
    "encoder3 = EncoderLSTM1(EMBEDDING_DIM, hidden_size, 11, 10, BATCH_SIZE, 2)\n",
    "attn_decoder3 = AttnDecoderRNN3(hidden_size, 10, 2)\n",
    "\n",
    "trainIters1(encoder3, attn_decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th 100 batches, average loss: 1.9746804237365723, average accuracy: 0.326875\n",
      "1th 100 batches, average loss: 1.8767398595809937, average accuracy: 0.3390625\n",
      "2th 100 batches, average loss: 1.7952020168304443, average accuracy: 0.3634375\n",
      "3th 100 batches, average loss: 1.8201353549957275, average accuracy: 0.3471875\n",
      "4th 100 batches, average loss: 1.7820048332214355, average accuracy: 0.3465625\n",
      "5th 100 batches, average loss: 1.535702109336853, average accuracy: 0.4103125\n",
      "6th 100 batches, average loss: 1.3702223300933838, average accuracy: 0.4715625\n",
      "7th 100 batches, average loss: 1.3061786890029907, average accuracy: 0.49875\n",
      "8th 100 batches, average loss: 1.2352449893951416, average accuracy: 0.52\n",
      "9th 100 batches, average loss: 1.1664303541183472, average accuracy: 0.5353125\n",
      "10th 100 batches, average loss: 1.134343147277832, average accuracy: 0.5615625\n",
      "11th 100 batches, average loss: 1.1096116304397583, average accuracy: 0.5721875\n",
      "12th 100 batches, average loss: 1.0959386825561523, average accuracy: 0.569375\n",
      "13th 100 batches, average loss: 1.0070801973342896, average accuracy: 0.61125\n",
      "14th 100 batches, average loss: 1.0020742416381836, average accuracy: 0.608125\n",
      "15th 100 batches, average loss: 0.9423825144767761, average accuracy: 0.6296875\n",
      "16th 100 batches, average loss: 0.8790721297264099, average accuracy: 0.6615625\n",
      "17th 100 batches, average loss: 0.8748158812522888, average accuracy: 0.664375\n",
      "18th 100 batches, average loss: 0.8457880616188049, average accuracy: 0.6753125\n",
      "19th 100 batches, average loss: 0.8318406939506531, average accuracy: 0.6721875\n",
      "20th 100 batches, average loss: 0.7984386682510376, average accuracy: 0.700625\n",
      "21th 100 batches, average loss: 0.7955799102783203, average accuracy: 0.699375\n",
      "22th 100 batches, average loss: 0.7738083004951477, average accuracy: 0.705625\n",
      "23th 100 batches, average loss: 0.717630922794342, average accuracy: 0.7221875\n",
      "24th 100 batches, average loss: 0.7054432034492493, average accuracy: 0.7325\n",
      "25th 100 batches, average loss: 0.7250058054924011, average accuracy: 0.7078125\n",
      "26th 100 batches, average loss: 0.6750420331954956, average accuracy: 0.744375\n",
      "27th 100 batches, average loss: 0.6762126088142395, average accuracy: 0.7325\n",
      "28th 100 batches, average loss: 0.6754186153411865, average accuracy: 0.74125\n",
      "29th 100 batches, average loss: 0.6758681535720825, average accuracy: 0.738125\n",
      "30th 100 batches, average loss: 0.6537573933601379, average accuracy: 0.7428125\n",
      "31th 100 batches, average loss: 0.6473478078842163, average accuracy: 0.7446875\n",
      "32th 100 batches, average loss: 0.6340578198432922, average accuracy: 0.7475\n",
      "33th 100 batches, average loss: 0.6595646739006042, average accuracy: 0.7390625\n",
      "34th 100 batches, average loss: 0.5986752510070801, average accuracy: 0.7615625\n",
      "35th 100 batches, average loss: 0.5912673473358154, average accuracy: 0.775\n",
      "36th 100 batches, average loss: 0.590391218662262, average accuracy: 0.766875\n",
      "37th 100 batches, average loss: 0.6014416217803955, average accuracy: 0.766875\n",
      "38th 100 batches, average loss: 0.6012163758277893, average accuracy: 0.765625\n",
      "39th 100 batches, average loss: 0.5962607860565186, average accuracy: 0.76375\n",
      "40th 100 batches, average loss: 0.579343318939209, average accuracy: 0.7725\n",
      "41th 100 batches, average loss: 0.5932268500328064, average accuracy: 0.7675\n",
      "42th 100 batches, average loss: 0.5680835843086243, average accuracy: 0.7803125\n",
      "43th 100 batches, average loss: 0.5515115261077881, average accuracy: 0.7784375\n",
      "44th 100 batches, average loss: 0.5770243406295776, average accuracy: 0.7728125\n",
      "45th 100 batches, average loss: 0.5749716758728027, average accuracy: 0.7796875\n",
      "46th 100 batches, average loss: 0.5595954060554504, average accuracy: 0.781875\n",
      "47th 100 batches, average loss: 0.5239847898483276, average accuracy: 0.7996875\n",
      "48th 100 batches, average loss: 0.5174430012702942, average accuracy: 0.796875\n",
      "49th 100 batches, average loss: 0.5219875574111938, average accuracy: 0.800625\n"
     ]
    }
   ],
   "source": [
    "class EncoderLSTM1(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, idx_size, batch_size, num_layers):\n",
    "        super(EncoderLSTM1, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to index space\n",
    "        self.hidden2idx = nn.Linear(hidden_dim, idx_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, seq, hidden):\n",
    "        embeds = self.embeddings(seq)\n",
    "        out, hidden = self.lstm(embeds, hidden) \n",
    "        return out, hidden\n",
    "\n",
    "class AttnDecoderRNN3(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, max_length=10):\n",
    "        super(AttnDecoderRNN3, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden_concat = torch.cat((encoder_outputs, hidden[0][-1].unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)), -1)\n",
    "        attn_weights = F.softmax(self.attn(hidden_concat), dim=-1).transpose(1, 2)\n",
    "        attn_applied = torch.bmm(attn_weights,\n",
    "                                 encoder_outputs)\n",
    "        output = F.relu(attn_applied)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        return output, hidden[0], attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "def trainIters1(encoder, decoder, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = total_correct = total_sample = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            indicator = i % 100\n",
    "            input_tensor, target_tensor = data\n",
    "            \n",
    "            encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            input_length = 11\n",
    "            target_length = 10\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(\n",
    "                    input_tensor, encoder_hidden)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "#             print(decoder_hidden[0].size(), decoder_hidden[1].size())\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            predicted = torch.argmax(decoder_output[:, -1, :], -1)\n",
    "            total_correct += (predicted == target_tensor).sum().item()\n",
    "            total_sample += target_tensor.size(0)\n",
    "            \n",
    "            loss += criterion(decoder_output[:, -1, :], target_tensor)\n",
    "            total_loss += loss\n",
    "            if indicator == 99:\n",
    "                ith = int(i/100)\n",
    "                avg_loss = total_loss/(indicator+1)\n",
    "                avg_acc = total_correct/total_sample\n",
    "                print(\"{}th 100 batches, average loss: {}, average accuracy: {}\".format(ith, avg_loss, avg_acc))\n",
    "                total_loss = total_correct = total_sample = 0\n",
    "                \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "hidden_size = 128\n",
    "encoder3 = EncoderLSTM1(EMBEDDING_DIM, hidden_size, 11, 10, BATCH_SIZE, 2)\n",
    "attn_decoder3 = AttnDecoderRNN3(hidden_size, 10, 2)\n",
    "\n",
    "trainIters1(encoder3, attn_decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    predicted = lstm_cr(inputs)\n",
    "    print(\"Predicted:\\n{}\\n\\nActual:\\n{}\".format(predicted, idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
