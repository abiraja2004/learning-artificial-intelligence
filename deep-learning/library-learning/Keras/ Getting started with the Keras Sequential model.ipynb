{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with the Keras Sequential model\n",
    "\n",
    "The Sequential model is a linear stack of layers.\n",
    "\n",
    "You can create a Sequential model by passing a list of layer instances to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784, )),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax',)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also simply add layers via the .add() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the input shape\n",
    "The model needs to know what input shape it should expect. For this reason, the first layer in a `Sequential` model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n",
    "\n",
    "- Pass an `input_shape` argument to the first layer. This is a shape tuple (a tuple of integers or  None entries, where None indicates that any positive integer may be expected). In  input_shape, the batch dimension is not included.\n",
    "- Some 2D layers, such as `Dense`, support the specification of their input shape via the argument  `input_dim`, and some 3D temporal layers support the arguments input_dim and  `input_length`.\n",
    "- If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a `batch_size` argument to a layer. If you pass both `batch_size=32` and `input_shape=(6, 8)` to a layer, it will then expect every batch of inputs to have the batch shape `(32, 6, 8)`.\n",
    "\n",
    "As such, the following snippets are strictly equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Sequential()\n",
    "model.add(Dense(32, input_dim=784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "Before training a model, you need to configure the learning process, which is done via the  `compile` method. It receives three arguments:\n",
    "\n",
    "- An optimizer. This could be the string identifier of an existing optimizer (such as `rmsprop` or  `adagrad`), or an instance of the `Optimize`r class. See: [optimizers](https://keras.io/optimizers).\n",
    "- A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as `categorical_crossentropy` or `mse`), or it can be an objective function. See: [losses](https://keras.io/losses).\n",
    "- A list of metrics. For any classification problem you will want to set this to  `metrics=['accuracy']`. A metric could be the string identifier of an existing metric or a custom metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='mse')\n",
    "\n",
    "# For custom metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy', mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the `fit` function. [Read its documentation here](https://keras.io/models/sequential/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.7201 - acc: 0.4850\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.7075 - acc: 0.5050\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.7007 - acc: 0.4970\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.6942 - acc: 0.5300\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.6898 - acc: 0.5360\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.6814 - acc: 0.5620\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.6791 - acc: 0.5490\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.6753 - acc: 0.5740\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.6703 - acc: 0.5760\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.6678 - acc: 0.5950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a77fd0390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 469us/step - loss: 2.3529 - acc: 0.1010\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 2.3121 - acc: 0.1080\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 2.3035 - acc: 0.1160\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 2.2897 - acc: 0.1290\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 2.2808 - acc: 0.1290\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 2.2714 - acc: 0.1600\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 2.2648 - acc: 0.1520\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 2.2552 - acc: 0.1560\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 2.2462 - acc: 0.1760\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 2.2369 - acc: 0.1830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a78787ac8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "import keras\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Here are a few examples to get you started!\n",
    "\n",
    "In the [examples folder](https://github.com/keras-team/keras/tree/master/examples), you will also find example models for real datasets:\n",
    "\n",
    "- CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation\n",
    "- IMDB movie review sentiment classification: LSTM over sequences of words\n",
    "- Reuters newswires topic classification: Multilayer Perceptron (MLP)\n",
    "- MNIST handwritten digits classification: MLP & CNN\n",
    "- Character-level text generation with LSTM\n",
    "\n",
    "...and more.\n",
    "\n",
    "### Multilayer Perceptron (MLP) for multi-class softmax classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 485us/step - loss: 2.3803 - acc: 0.1130\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 2.3683 - acc: 0.1030\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 2.3490 - acc: 0.1090\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 2.3408 - acc: 0.1110\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 2.3351 - acc: 0.0990\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 2.3123 - acc: 0.1080\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 2.3200 - acc: 0.1020\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 2.3086 - acc: 0.0930\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 2.3112 - acc: 0.1150\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.3125 - acc: 0.1100\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 2.3078 - acc: 0.1100\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 2.3053 - acc: 0.1180\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 2.3002 - acc: 0.1210\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 2.3092 - acc: 0.1160\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 2.3096 - acc: 0.1090\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.2990 - acc: 0.0950\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 2.2998 - acc: 0.1130\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 2.3025 - acc: 0.1050\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.3034 - acc: 0.0990\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 2.2975 - acc: 0.1030\n",
      "100/100 [==============================] - 0s 740us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=sgd,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         epochs=20,\n",
    "         batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 804us/step - loss: 0.7004 - acc: 0.5390\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.7134 - acc: 0.4920\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.7058 - acc: 0.4880\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.6954 - acc: 0.5330\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.7060 - acc: 0.4980\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.6995 - acc: 0.5190\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.6990 - acc: 0.5090\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.7025 - acc: 0.5150\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.6939 - acc: 0.5290\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.6964 - acc: 0.4990\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.6913 - acc: 0.5190\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.6982 - acc: 0.5020\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.6887 - acc: 0.5440\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.6944 - acc: 0.5130\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.6934 - acc: 0.5210\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.6953 - acc: 0.5020\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.6956 - acc: 0.5180\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.6876 - acc: 0.5520\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.6890 - acc: 0.5220\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.6933 - acc: 0.5330\n",
      "100/100 [==============================] - 0s 462us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         epochs=20,\n",
    "         batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  VGG-like convnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 2.3361\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 2.3826\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 2.2826\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 2.2882\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 2.2964\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 2.3146\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 2.3004\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 2.2883\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 2.2931\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 2.2998\n",
      "20/20 [==============================] - 1s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Sequence classification with LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.6936 - acc: 0.5020\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5972 - acc: 0.7340\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3949 - acc: 0.8390\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2531 - acc: 0.9060\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1545 - acc: 0.9460\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0697 - acc: 0.9770\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0263 - acc: 0.9900\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 6.1395e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0010 - acc: 1.0000\n",
      "100/100 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "max_features = 1000  # Size of the vocabulary\n",
    "x_train = np.random.randint(max_features, size=(1000, 10))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.randint(max_features, size=(100, 10))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence classification with LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6957 - acc: 0.4910\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.6905 - acc: 0.5310\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5985 - acc: 0.6920\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3849 - acc: 0.8450\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.1754 - acc: 0.9490\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.1223 - acc: 0.9610\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0448 - acc: 0.9860\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0502 - acc: 0.9850\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0294 - acc: 0.9910\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0703 - acc: 0.9840\n",
      "100/100 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPool1D\n",
    "\n",
    "seq_length = 1000  # Size of the vocabulary\n",
    "x_train = np.random.randint(seq_length, size=(1000, 100))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.randint(seq_length, size=(100, 100))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(seq_length, output_dim=256))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPool1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM for sequence classification\n",
    "In this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations.\n",
    "\n",
    "The first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector).\n",
    "\n",
    "![Stacked LSTM](https://keras.io/img/regular_stacked_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 11.4854 - acc: 0.0960 - val_loss: 11.2499 - val_acc: 0.1000\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 315us/step - loss: 11.4836 - acc: 0.1130 - val_loss: 11.2495 - val_acc: 0.1000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 321us/step - loss: 11.4830 - acc: 0.1220 - val_loss: 11.2488 - val_acc: 0.1700\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 309us/step - loss: 11.4826 - acc: 0.1110 - val_loss: 11.2476 - val_acc: 0.1500\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 313us/step - loss: 11.4823 - acc: 0.1200 - val_loss: 11.2481 - val_acc: 0.1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a12487fd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "              input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "         batch_size=64, epochs=5,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same stacked LSTM model, rendered \"stateful\"\n",
    "A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable.\n",
    "\n",
    "[You can read more about stateful RNNs in the FAQ.](https://keras.io/getting-started/faq/#how-can-i-use-stateful-rnns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 96 samples\n",
      "Epoch 1/5\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 11.6284 - acc: 0.0875 - val_loss: 11.3278 - val_acc: 0.0938\n",
      "Epoch 2/5\n",
      "320/320 [==============================] - 0s 470us/step - loss: 11.6204 - acc: 0.1125 - val_loss: 11.3280 - val_acc: 0.0938\n",
      "Epoch 3/5\n",
      "320/320 [==============================] - 0s 478us/step - loss: 11.6190 - acc: 0.1062 - val_loss: 11.3282 - val_acc: 0.1250\n",
      "Epoch 4/5\n",
      "320/320 [==============================] - 0s 472us/step - loss: 11.6178 - acc: 0.1250 - val_loss: 11.3285 - val_acc: 0.1354\n",
      "Epoch 5/5\n",
      "320/320 [==============================] - 0s 456us/step - loss: 11.6164 - acc: 0.1344 - val_loss: 11.3290 - val_acc: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a1c175c88>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "              batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(32, stateful=True))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "                      \n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "y_train = np.random.random((batch_size * 10, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "y_val = np.random.random((batch_size * 3, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "         batch_size=batch_size, epochs=5, shuffle=False,\n",
    "         validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
