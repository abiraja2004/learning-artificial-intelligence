{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Quick Start\n",
    "\n",
    "The [tf.data](https://tensorflow.google.cn/api_docs/python/tf/data) module contains a collection of classes that allows you to easily load data, manipulate it, and pipe it into your model. This document introduces the API by walking through two simple examples:\n",
    "\n",
    "- Reading in-memory data from numpy arrays.\n",
    "- Reading lines from a csv file.\n",
    "\n",
    "## Basic input\n",
    "\n",
    "Taking slices from an array is the simplest way to get started with [tf.data](https://tensorflow.google.cn/api_docs/python/tf/data).\n",
    "\n",
    "The [Premade Estimators](https://tensorflow.google.cn/get_started/premade_estimators) chapter describes the following train_input_fn, from [iris_data.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py), to pipe the data into the Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    \n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments\n",
    "\n",
    "This function expects three arguments. Arguments expecting an \"array\" can accept nearly anything that can be converted to an array with numpy.array. One exception is [tuple](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences) which, as we will see, has special meaning for Datasets.\n",
    "\n",
    "- features: A {'feature_name':array} dictionary (or [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)) containing the raw input features.\n",
    "- labels : An array containing the [label](https://developers.google.cn/machine-learning/glossary/#label) for each example.\n",
    "- batch_size : An integer indicating the desired batch size.\n",
    "\n",
    "In [premade_estimator.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py) we retrieved the Iris data using the iris_data.load_data() function. You can run it, and unpack the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris_data\n",
    "\n",
    "# Fetch the data\n",
    "train, test = iris_data.load_data()\n",
    "features, labels = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we passed this data to the input function, with a line similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "iris_data.train_input_fn(features, lables, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the train_input_fn().\n",
    "\n",
    "### Slices\n",
    "\n",
    "The function starts by using the [tf.data.Dataset.from_tensor_slices](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#from_tensor_slices) function to create a [tf.data.Dataset](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset) representing slices of the array. The array is sliced across the first dimension. For example, an array containing the [mnist training data](https://tensorflow.google.cn/tutorials/layers) has a shape of (60000, 28, 28). Passing this to from_tensor_slices returns a Dataset object containing 60000 slices, each one a 28x28 image.\n",
    "\n",
    "The code that returns this Dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (28, 28), types: tf.uint8>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "mnist_x, minist_y = train\n",
    "\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices(mnist_x)\n",
    "print(mnist_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print the above output, showing the shapes and types of the items in the dataset. Note that a Dataset does not know how many items it contains.\n",
    "\n",
    "The Dataset above represents a simple collection of arrays, but datasets are much more powerful than this. A Dataset can transparently handle any nested combination of dictionaries or tuples (or [namedtuple](https://docs.python.org/2/library/collections.html#collections.namedtuple) ).\n",
    "\n",
    "For example after converting the iris features to a standard python dictionary, you can then convert the dictionary of arrays to a Dataset of dictionaries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: {SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, types: {SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(dict(features))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that when a Dataset contains structured elements, the shapes and types of the Dataset take on the same structure. This dataset contains dictionaries of [scalars](https://tensorflow.google.cn/programmers_guide/tensors#rank), all of type [tf.float64](https://tensorflow.google.cn/api_docs/python/tf/double).\n",
    "\n",
    "The first line of the iris train_input_fn uses the same functionality, but adds another level of structure. It creates a dataset containing (features_dict, label) pairs.\n",
    "\n",
    "The following code shows that the label is a scalar with type int64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# Convert the inputs to a Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation\n",
    "Currently the Dataset would iterate over the data once, in a fixed order, and only produce a single element at a time. It needs further processing before it can be used for training. Fortunately, the [tf.data.Dataset](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset) class provides methods to better prepare the data for training. The next line of the input function takes advantage of several of these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle, repeat, and batch the examples.\n",
    "dataset = dataset.shuffle(1000).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [shuffle](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#shuffle) method uses a fixed-size buffer to shuffle the items as they pass through. In this case the buffer_size is greater than the number of examples in the Dataset, ensuring that the data is completely shuffled (The Iris data set only contains 150 examples).\n",
    "\n",
    "The [repeat](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#repeat) method restarts the Dataset when it reaches the end. To limit the number of epochs, set the count argument.\n",
    "\n",
    "The [batch](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#batch) method collects a number of examples and stacks them, to create batches. This adds a dimension to their shape. The new dimension is added as the first dimension. The following code uses the batch method on the MNIST Dataset, from earlier. This results in a Dataset containing 3D arrays representing stacks of (28,28) images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (?, 28, 28), types: tf.uint8>\n"
     ]
    }
   ],
   "source": [
    "print(mnist_ds.batch(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset has an unknown batch size because the last batch will have fewer elements.\n",
    "\n",
    "In train_input_fn, after batching the Dataset contains 1D vectors of elements where each scalar was previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return\n",
    "\n",
    "At this point the Dataset contains (features_dict, labels) pairs. This is the format expected by the train and evaluate methods, so the input_fn returns the dataset.\n",
    "\n",
    "The labels can/should be omitted when using the predict method.\n",
    "\n",
    "## Reading a CSV File\n",
    "\n",
    "The most common real-world use case for the Dataset class is to stream data from files on disk. The [tf.data](https://tensorflow.google.cn/api_docs/python/tf/data) module includes a variety of file readers. Let's see how parsing the Iris dataset from the csv file looks using a Dataset.\n",
    "\n",
    "The following call to the iris_data.maybe_download function downloads the data if necessary, and returns the pathnames of the resulting files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path, test_path = iris_data.maybe_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris_data.csv_input_fn function contains an alternative implementation that parses the csv files using a Dataset.\n",
    "\n",
    "Let's look at how to build an Estimator-compatible input function that reads from the local files.\n",
    "\n",
    "### Build the Dataset\n",
    "\n",
    "We start by building a [TextLineDataset](https://tensorflow.google.cn/api_docs/python/tf/data/TextLineDataset) object to read the file one line at a time. Then, we call the [skip](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#skip) method to skip over the first line of the file, which contains a header, not an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(train_path).skip(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a csv line parser\n",
    "\n",
    "We will start by building a function to parse a single line.\n",
    "\n",
    "The following iris_data.parse_line function accomplishes this task using the [tf.decode_csv](https://tensorflow.google.cn/api_docs/python/tf/decode_csv) function, and some simple python code:\n",
    "\n",
    "We must parse each of the lines in the dataset in order to generate the necessary (features, label) pairs. The following _parse_line function calls [tf.decode_csv](https://tensorflow.google.cn/api_docs/python/tf/decode_csv) to parse a single line into its features and the label. Since Estimators require that features be represented as a dictionary, we rely on Python's built-in dict and zip functions to build that dictionary. The feature names are the keys of that dictionary. We then call the dictionary's pop method to remove the label field from the features dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Metadata describing the text columns\n",
    "COLUMNS = ['SepalLength', 'SepalWidth',\n",
    "           'PetalLength', 'PetalWidth',\n",
    "           'label']\n",
    "FIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, FIELD_DEFAULTS)\n",
    "    \n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(COLUMNS, fields))\n",
    "    \n",
    "    # Separate the label from the features\n",
    "    label = features.pop('label')\n",
    "    \n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the lines\n",
    "\n",
    "Datasets have many methods for manipulating the data while it is being piped to a model. The most heavily-used method is [map](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#map), which applies a transformation to each element of the Dataset.\n",
    "\n",
    "The map method takes a map_func argument that describes how each item in the Dataset should be transformed.\n",
    "\n",
    "![](https://tensorflow.google.cn/images/datasets/map.png)\n",
    "<center>The [map](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#map) method applies the `map_func` to transform each item in the Dataset.</center>\n",
    "\n",
    "So to parse the lines as they are streamed out of the csv file, we pass our _parse_line function to the map method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float32, SepalWidth: tf.float32, PetalLength: tf.float32, PetalWidth: tf.float32}, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(_parse_line)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of simple scalar strings, the dataset contains (features, label) pairs.\n",
    "\n",
    "### Try it out\n",
    "This function can be used as a replacement for iris_data.train_input_fn. It can be used to feed an estimator as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpj91qtsyb\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\ADMINI~1\\\\AppData\\\\Local\\\\Temp\\\\tmpj91qtsyb', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026D4E95D518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpj91qtsyb\\model.ckpt.\n",
      "INFO:tensorflow:loss = 109.86121, step = 1\n",
      "INFO:tensorflow:global_step/sec: 99.8619\n",
      "INFO:tensorflow:loss = 37.640728, step = 101 (1.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.305\n",
      "INFO:tensorflow:loss = 25.001226, step = 201 (0.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.758\n",
      "INFO:tensorflow:loss = 21.537903, step = 301 (0.855 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.122\n",
      "INFO:tensorflow:loss = 19.11237, step = 401 (0.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.014\n",
      "INFO:tensorflow:loss = 15.314343, step = 501 (0.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6656\n",
      "INFO:tensorflow:loss = 14.455785, step = 601 (1.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.05\n",
      "INFO:tensorflow:loss = 15.083953, step = 701 (0.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.443\n",
      "INFO:tensorflow:loss = 15.774256, step = 801 (0.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.168\n",
      "INFO:tensorflow:loss = 12.916412, step = 901 (0.854 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpj91qtsyb\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 10.772078.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x26d526baf28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path, test_path = iris_data.maybe_download()\n",
    "\n",
    "# All the inputs are numeric\n",
    "feature_columns = [\n",
    "    tf.feature_column.numeric_column(name)\n",
    "    for name in iris_data.CSV_COLUMN_NAMES[:-1]]\n",
    "\n",
    "# Build the estimator\n",
    "est = tf.estimator.LinearClassifier(feature_columns, n_classes=3)\n",
    "# Train the estimator\n",
    "batch_size = 100\n",
    "est.train(\n",
    "    steps=1000,\n",
    "    input_fn=lambda:iris_data.csv_input_fn(train_path, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators expect an input_fn to take no arguments. To work around this restriction, we use lambda to capture the arguments and provide the expected interface.\n",
    "\n",
    "## Summary\n",
    "The [tf.data](https://tensorflow.google.cn/api_docs/python/tf/data) module provides a collection of classes and functions for easily reading data from a variety of sources. Furthermore, [tf.data](https://tensorflow.google.cn/api_docs/python/tf/data) has simple powerful methods for applying a wide variety of standard and custom transformations.\n",
    "\n",
    "Now you have the basic idea of how to efficiently load data into an Estimator. Consider the following documents next:\n",
    "\n",
    "- [Creating Custom Estimators](https://tensorflow.google.cn/get_started/custom_estimators), which demonstrates how to build your own custom Estimator model.\n",
    "- The [Low Level Introduction](https://tensorflow.google.cn/programmers_guide/low_level_intro#datasets), which demonstrates how to experiment directly with tf.data.Datasets using TensorFlow's low level APIs.\n",
    "- [Importing Data](https://tensorflow.google.cn/programmers_guide/datasets) which goes into great detail about additional functionality of Datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
