{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard: Visualizing Learning\n",
    "\n",
    "The computations you'll use TensorFlow for - like training a massive\n",
    "deep neural network - can be complex and confusing. To make it easier to\n",
    "understand, debug, and optimize TensorFlow programs, we've included a suite of\n",
    "visualization tools called TensorBoard. You can use TensorBoard to visualize\n",
    "your TensorFlow graph, plot quantitative metrics about the execution of your\n",
    "graph, and show additional data like images that pass through it. When\n",
    "TensorBoard is fully configured, it looks like this:\n",
    "\n",
    "![MNIST TensorBoard](https://tensorflow.google.cn/images/mnist_tensorboard.png \"MNIST TensorBoard\")\n",
    "\n",
    "<div class=\"video-wrapper\">\n",
    "  <iframe class=\"devsite-embedded-youtube-video\" data-video-id=\"eBbEDRsCmv4\"\n",
    "          data-autohide=\"1\" data-showinfo=\"0\" frameborder=\"0\" allowfullscreen>\n",
    "  </iframe>\n",
    "</div>\n",
    "\n",
    "This 30-minute tutorial is intended to get you started with simple TensorBoard\n",
    "usage. It assumes a basic understanding of TensorFlow.\n",
    "\n",
    "There are other resources available as well! The [TensorBoard GitHub](https://github.com/tensorflow/tensorboard)\n",
    "has a lot more information on using individual dashboards within TensorBoard\n",
    "including tips & tricks and debugging information.\n",
    "\n",
    "## Setup\n",
    "\n",
    "[Install TensorFlow](https://www.tensorflow.org/install/). Installing TensorFlow\n",
    "via pip should also automatically install TensorBoard.\n",
    "\n",
    "## Serializing the data\n",
    "\n",
    "TensorBoard operates by reading TensorFlow events files, which contain summary\n",
    "data that you can generate when running TensorFlow. Here's the general\n",
    "lifecycle for summary data within TensorBoard.\n",
    "\n",
    "First, create the TensorFlow graph that you'd like to collect summary\n",
    "data from, and decide which nodes you would like to annotate with\n",
    "[summary operations](https://tensorflow.google.cn/api_guides/python/summary).\n",
    "\n",
    "For example, suppose you are training a convolutional neural network for\n",
    "recognizing MNIST digits. You'd like to record how the learning rate\n",
    "varies over time, and how the objective function is changing. Collect these by\n",
    "attaching [tf.summary.scalar](https://tensorflow.google.cn/api_docs/python/tf/summary/scalar) ops\n",
    "to the nodes that output the learning rate and loss respectively. Then, give\n",
    "each `scalar_summary` a meaningful `tag`, like `'learning rate'` or `'loss\n",
    "function'`.\n",
    "\n",
    "Perhaps you'd also like to visualize the distributions of activations coming\n",
    "off a particular layer, or the distribution of gradients or weights. Collect\n",
    "this data by attaching\n",
    "[tf.summary.histogram](https://tensorflow.google.cn/api_docs/python/tf/summary/histogram) ops to\n",
    "the gradient outputs and to the variable that holds your weights, respectively.\n",
    "\n",
    "For details on all of the summary operations available, check out the docs on\n",
    "[summary operations](https://tensorflow.google.cn/api_guides/python/summary).\n",
    "\n",
    "Operations in TensorFlow don't do anything until you run them, or an op that\n",
    "depends on their output. And the summary nodes that we've just created are\n",
    "peripheral to your graph: none of the ops you are currently running depend on\n",
    "them. So, to generate summaries, we need to run all of these summary nodes.\n",
    "Managing them by hand would be tedious, so use\n",
    "[tf.summary.merge_all](https://tensorflow.google.cn/api_docs/python/tf/summary/merge_all)\n",
    "to combine them into a single op that generates all the summary data.\n",
    "\n",
    "Then, you can just run the merged summary op, which will generate a serialized\n",
    "`Summary` protobuf object with all of your summary data at a given step.\n",
    "Finally, to write this summary data to disk, pass the summary protobuf to a\n",
    "[tf.summary.FileWriter](https://tensorflow.google.cn/api_docs/python/tf/summary/FileWriter).\n",
    "\n",
    "The `FileWriter` takes a logdir in its constructor - this logdir is quite\n",
    "important, it's the directory where all of the events will be written out.\n",
    "Also, the `FileWriter` can optionally take a `Graph` in its constructor.\n",
    "If it receives a `Graph` object, then TensorBoard will visualize your graph\n",
    "along with tensor shape information. This will give you a much better sense of\n",
    "what flows through the graph: see\n",
    "[Tensor shape information](https://tensorflow.google.cn/programmers_guide/graph_viz#tensor-shape-information).\n",
    "\n",
    "Now that you've modified your graph and have a `FileWriter`, you're ready to\n",
    "start running your network! If you want, you could run the merged summary op\n",
    "every single step, and record a ton of training data. That's likely to be more\n",
    "data than you need, though. Instead, consider running the merged summary op\n",
    "every `n` steps.\n",
    "\n",
    "The code example below is a modification of the\n",
    "[simple MNIST tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py),\n",
    "in which we have added some summary ops, and run them every ten steps. If you\n",
    "run this and then launch `tensorboard --logdir=/tmp/tensorflow/mnist`, you'll be able\n",
    "to visualize statistics, such as how the weights or accuracy varied during\n",
    "training. The code below is an excerpt; full source is\n",
    "[here](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope(\"Wx_plus_b\"):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations\n",
    "    \n",
    "hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "    \n",
    "# Do not apply softmax activation yet, see below.\n",
    "y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    # The raw formulation of cross-entropy,\n",
    "    #\n",
    "    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "    #                               reduction_indices=[1]))\n",
    "    #\n",
    "    # can be numerically unstable.\n",
    "    #\n",
    "    # So here we use tf.losses.sparse_softmax_cross_entropy on the\n",
    "    # raw logit outputs of the nn_layer above.\n",
    "    with tf.name_scope('total'):\n",
    "        cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "      train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n",
    "          cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                      sess.graph)\n",
    "test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/test')\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've initialized the `FileWriters`, we have to add summaries to the\n",
    "`FileWriters` as we train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model, and also write summaries.\n",
    "# Every 10th step, measure test-set accuracy, and write test summaries\n",
    "# All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "def feed_dict(train):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train or FLAGS.fake_data:\n",
    "        xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
    "        k = FLAGS.dropout\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        k = 1.0\n",
    "    return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "for i in range(FLAGS.max_steps):\n",
    "    if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print('Accuracy at step %s: %s' % (i, acc))\n",
    "    else:  # Record train set summaries, and train\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "        train_writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now all set to visualize this data using TensorBoard.\n",
    "\n",
    "\n",
    "## Launching TensorBoard\n",
    "\n",
    "To run TensorBoard, use the following command (alternatively `python -m tensorboard.main`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard --logdir=path/to/log-directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `logdir` points to the directory where the `FileWriter` serialized its\n",
    "data.  If this `logdir` directory contains subdirectories which contain\n",
    "serialized data from separate runs, then TensorBoard will visualize the data\n",
    "from all of those runs. Once TensorBoard is running, navigate your web browser\n",
    "to `localhost:6006` to view the TensorBoard.\n",
    "\n",
    "When looking at TensorBoard, you will see the navigation tabs in the top right\n",
    "corner. Each tab represents a set of serialized data that can be visualized.\n",
    "\n",
    "For in depth information on how to use the *graph* tab to visualize your graph,\n",
    "see [Graph Visualization](https://tensorflow.google.cn/programmers_guide/graph_viz).\n",
    "\n",
    "For more usage information on TensorBoard in general, see the\n",
    "[TensorBoard GitHub](https://github.com/tensorflow/tensorboard)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
