{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from textacy.vsm import Vectorizer\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from file\n",
    "tweets = pd.read_csv('tweets.txt', sep='\\t',\n",
    "                        names=['id', 'topic', 'sentiment', 'body'])  # Change here to your own column name\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628949369883000832</td>\n",
       "      <td>@microsoft</td>\n",
       "      <td>negative</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628976607420645377</td>\n",
       "      <td>@microsoft</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>629023169169518592</td>\n",
       "      <td>@microsoft</td>\n",
       "      <td>negative</td>\n",
       "      <td>I may be ignorant on this issue but... should ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>629179223232479232</td>\n",
       "      <td>@microsoft</td>\n",
       "      <td>negative</td>\n",
       "      <td>Thanks to @microsoft, I just may be switching ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>629226490152914944</td>\n",
       "      <td>@microsoft</td>\n",
       "      <td>positive</td>\n",
       "      <td>Microsoft, I may not prefer your gaming branch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id       topic sentiment  \\\n",
       "0  628949369883000832  @microsoft  negative   \n",
       "1  628976607420645377  @microsoft  negative   \n",
       "2  629023169169518592  @microsoft  negative   \n",
       "3  629179223232479232  @microsoft  negative   \n",
       "4  629226490152914944  @microsoft  positive   \n",
       "\n",
       "                                                body  \n",
       "0  dear @Microsoft the newOoffice for Mac is grea...  \n",
       "1  @Microsoft how about you make a system that do...  \n",
       "2  I may be ignorant on this issue but... should ...  \n",
       "3  Thanks to @microsoft, I just may be switching ...  \n",
       "4  Microsoft, I may not prefer your gaming branch...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At beginning, sentence level tokenization should be done since our task objective is sentence extraction.\n",
    "\n",
    "The second step is cleaning data. Intuitively, those string like url, '@...', hashtag and punctuation seldom contribute to the importance of sentence. In addition, stopwords are usually regarded as noises in most of the NLP tasks. These things should be removed.\n",
    "\n",
    "Finally, a filtered word list following the original sentence sequence is created for tf-idf value calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    '''\n",
    "    Clean input data and tokenize them for tf-idf calculation.\n",
    "\n",
    "    :param data: input tweets\n",
    "    :return: sents, a tokenized sentences list.\n",
    "            filtered_words, a tokenized sentences list whose element is cleaned word list(e.g. [['microsoft', 'vista'], ...]）\n",
    "    '''\n",
    "    sents = [sent for tweet in data.body.tolist() for sent in nltk.sent_tokenize(tweet)]  # Tokenize sentence\n",
    "    cleaned_sents = sents.copy()  # Prepare a copy for cleaning\n",
    "    cleaned_sents = list(map(lambda x: re.sub('http\\S+', '', x), cleaned_sents))  # Removing URLS\n",
    "    cleaned_sents = list(map(lambda x: re.sub('(\\s)@\\w+', '', x), cleaned_sents))  # Removing @...\n",
    "    cleaned_sents = list(map(lambda x: re.sub('#', '', x), cleaned_sents))  # Removing hashtags\n",
    "    cleaned_sents = list(map(lambda x: x.translate(str.maketrans('', '', string.punctuation)), cleaned_sents))  # Removing puntuation\n",
    "    # Tokenization and Removing stopwords\n",
    "    tokenizer = TweetTokenizer()\n",
    "    filtered_words = [[word.lower() for word in tokenizer.tokenize(sent) if word.lower() not in stopwords.words('english')]\n",
    "        for sent in cleaned_sents]\n",
    "\n",
    "    return sents, filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:158\n",
      "['dear @Microsoft the newOoffice for Mac is great and all, but no Lync update?',\n",
      " \"C'mon.\",\n",
      " \"@Microsoft how about you make a system that doesn't eat my friggin discs.\",\n",
      " 'This is the 2nd time this has happened and I am so sick of it!',\n",
      " \"I may be ignorant on this issue but... should we celebrate @Microsoft's \"\n",
      " 'parental leave changes?']\n",
      "------------------------------------------------------------------------------------\n",
      "Number of unique words after filtering:591\n",
      "[['dear', 'newooffice', 'mac', 'great', 'lync', 'update'],\n",
      " ['cmon'],\n",
      " ['microsoft', 'make', 'system', 'doesnt', 'eat', 'friggin', 'discs'],\n",
      " ['2nd', 'time', 'happened', 'sick'],\n",
      " ['may', 'ignorant', 'issue', 'celebrates', 'parental', 'leave', 'changes']]\n"
     ]
    }
   ],
   "source": [
    "sents, filtered_words = preprocess(tweets.copy())\n",
    "print('Number of sentences:' + str(len(sents)))\n",
    "pp.pprint(sents[:5])\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print('Number of unique words after filtering:' + str(len(set([token for sent in filtered_words for token in sent]))))\n",
    "pp.pprint(filtered_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caculating tf-idf value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use [textacy](https://textacy.readthedocs.io/en/latest/index.html), a higher-level NLP built on spaCy, to calculate  tf-idf score. Since I only care about the meaningful token of each sentence, I apply this technique on the filtered word list created from previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(data_tokenized):\n",
    "    '''\n",
    "    Caculate tf-idf matrix.\n",
    "\n",
    "    :param data_tokenized: A sequence of tokenized documents, where each document is a sequence of (str) terms.\n",
    "    :return: vectorizer, instance of textacy.vsm.Vectorizer.\n",
    "            calculate , tf-idf matrix whose row is document, column is term\n",
    "    '''\n",
    "    vectorizer = Vectorizer(weighting='tfidf')\n",
    "    term_matrix = vectorizer.fit_transform(data_tokenized).todense()  # dense matrix means most of the elements are nonzero\n",
    "\n",
    "    return vectorizer, term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 591)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer, term_matrix = tfidf(filtered_words)\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mention in the code comment, the return term_matrix is a term‐document matrix which is also called \"bag‐of‐words\". At this case, the term_matrix contains 158 documents and 591 terms which is corresponding to the number of sentences and number of unique words after filtering we create in the Preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract most representative sentences as summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tweet is short, some widely-used techniques like position weights and biased heading weights are not consider suitable for the task. At current stage, the sum score of each word in sentence is used to rank sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences(sents, filtered_words, vectorizer, term_matrix, top_n=3):\n",
    "    '''\n",
    "    Select top n important sentence.\n",
    "\n",
    "    :param sents: a list containing sentences.\n",
    "    :param filtered_words: a tokenized sentences list whose element is word list\n",
    "    :param vectorizer: instance of textacy.vsm.Vectorizer\n",
    "    :param term_matrix: tf-idf matrix whose row is document, column is term\n",
    "    :param top_n: the selecting number\n",
    "    :return: a list containing top n important sentences\n",
    "    '''\n",
    "    tfidf_sent = [[term_matrix[index, vectorizer.vocabulary[token]] for token in sent] for index, sent in\n",
    "                  enumerate(filtered_words)]  # Get tfidf value for noun word in each sentence\n",
    "    sent_values = [sum(sent) for sent in tfidf_sent]  # Caculate whole tfidf weights for each sentence\n",
    "    ranked_sent = sorted(zip(sents, sent_values), key=lambda x: x[1], reverse=True)  # Sort sentence at descending order\n",
    "\n",
    "    return [sent[0] for sent in ranked_sent[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 3  # Select top n importance sentences\n",
    "best_sents = rank_sentences(sents, filtered_words, vectorizer, term_matrix, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@eyesonfoxorg @Microsoft I'm still using Vista on one &amp; Win-7 on \"\n",
      " 'another, Vista is a dinosaur, unfortunately I may use a free 10 with limits',\n",
      " 'W/ all the $$$ and drones U have working 4 U, maybe U guys could get it '\n",
      " 'right the 1st time?',\n",
      " \"@Lumia #Lumia @Microsoft 2nd, you guys haven't released a lumia that has a \"\n",
      " 'QHD screen, or takes video in 2k resolution yet.']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(best_sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
